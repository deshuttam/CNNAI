{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"xlsx_Uttam_NIST.ipynb","provenance":[],"collapsed_sections":[],"machine_shape":"hm"},"kernelspec":{"name":"python2","display_name":"Python 2"},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"id":"myk42bblMeUv","colab_type":"code","colab":{}},"source":["import os\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"Byj3zlj0hGck","colab_type":"code","outputId":"41080530-d4ad-4b92-d3ed-3b81d1d61cd8","executionInfo":{"status":"ok","timestamp":1585771415794,"user_tz":-330,"elapsed":4803,"user":{"displayName":"Uttam Deshpande","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gi0ntRxzuR6q-uoz9PehErcklTqufxtYeGtc1T69w=s64","userId":"14680885720442117314"}},"colab":{"base_uri":"https://localhost:8080/","height":493}},"source":["!pip install tensorflow==1.15"],"execution_count":2,"outputs":[{"output_type":"stream","text":["Requirement already satisfied: tensorflow==1.15 in /usr/local/lib/python2.7/dist-packages (1.15.0)\n","Requirement already satisfied: gast==0.2.2 in /usr/local/lib/python2.7/dist-packages (from tensorflow==1.15) (0.2.2)\n","Requirement already satisfied: grpcio>=1.8.6 in /usr/local/lib/python2.7/dist-packages (from tensorflow==1.15) (1.15.0)\n","Requirement already satisfied: numpy<2.0,>=1.16.0 in /usr/local/lib/python2.7/dist-packages (from tensorflow==1.15) (1.16.4)\n","Requirement already satisfied: mock>=2.0.0 in /usr/local/lib/python2.7/dist-packages (from tensorflow==1.15) (2.0.0)\n","Requirement already satisfied: tensorboard<1.16.0,>=1.15.0 in /usr/local/lib/python2.7/dist-packages (from tensorflow==1.15) (1.15.0)\n","Requirement already satisfied: protobuf>=3.6.1 in /usr/local/lib/python2.7/dist-packages (from tensorflow==1.15) (3.8.0)\n","Requirement already satisfied: keras-preprocessing>=1.0.5 in /usr/local/lib/python2.7/dist-packages (from tensorflow==1.15) (1.1.0)\n","Requirement already satisfied: backports.weakref>=1.0rc1; python_version < \"3.4\" in /usr/local/lib/python2.7/dist-packages (from tensorflow==1.15) (1.0.post1)\n","Requirement already satisfied: tensorflow-estimator==1.15.1 in /usr/local/lib/python2.7/dist-packages (from tensorflow==1.15) (1.15.1)\n","Requirement already satisfied: keras-applications>=1.0.8 in /usr/local/lib/python2.7/dist-packages (from tensorflow==1.15) (1.0.8)\n","Requirement already satisfied: wheel in /usr/local/lib/python2.7/dist-packages (from tensorflow==1.15) (0.34.2)\n","Requirement already satisfied: wrapt>=1.11.1 in /usr/local/lib/python2.7/dist-packages (from tensorflow==1.15) (1.11.2)\n","Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python2.7/dist-packages (from tensorflow==1.15) (1.12.0)\n","Requirement already satisfied: absl-py>=0.7.0 in /usr/local/lib/python2.7/dist-packages (from tensorflow==1.15) (0.7.1)\n","Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python2.7/dist-packages (from tensorflow==1.15) (2.3.2)\n","Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python2.7/dist-packages (from tensorflow==1.15) (1.1.0)\n","Requirement already satisfied: google-pasta>=0.1.6 in /usr/local/lib/python2.7/dist-packages (from tensorflow==1.15) (0.1.7)\n","Requirement already satisfied: enum34>=1.1.6; python_version < \"3.4\" in /usr/local/lib/python2.7/dist-packages (from tensorflow==1.15) (1.1.6)\n","Requirement already satisfied: functools32>=3.2.3 in /usr/local/lib/python2.7/dist-packages (from tensorflow==1.15) (3.2.3.post2)\n","Requirement already satisfied: astor>=0.6.0 in /usr/local/lib/python2.7/dist-packages (from tensorflow==1.15) (0.8.0)\n","Requirement already satisfied: futures>=2.2.0 in /usr/local/lib/python2.7/dist-packages (from grpcio>=1.8.6->tensorflow==1.15) (3.2.0)\n","Requirement already satisfied: funcsigs>=1; python_version < \"3.3\" in /usr/local/lib/python2.7/dist-packages (from mock>=2.0.0->tensorflow==1.15) (1.0.2)\n","Requirement already satisfied: pbr>=0.11 in /usr/local/lib/python2.7/dist-packages (from mock>=2.0.0->tensorflow==1.15) (5.4.0)\n","Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python2.7/dist-packages (from tensorboard<1.16.0,>=1.15.0->tensorflow==1.15) (44.0.0)\n","Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python2.7/dist-packages (from tensorboard<1.16.0,>=1.15.0->tensorflow==1.15) (0.15.5)\n","Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python2.7/dist-packages (from tensorboard<1.16.0,>=1.15.0->tensorflow==1.15) (3.1.1)\n","Requirement already satisfied: h5py in /usr/local/lib/python2.7/dist-packages (from keras-applications>=1.0.8->tensorflow==1.15) (2.8.0)\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"6K07dXojagaO","colab_type":"code","outputId":"a07cf6e0-c922-47c1-8cb5-6cfd0f542a22","executionInfo":{"status":"ok","timestamp":1585771418655,"user_tz":-330,"elapsed":7647,"user":{"displayName":"Uttam Deshpande","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gi0ntRxzuR6q-uoz9PehErcklTqufxtYeGtc1T69w=s64","userId":"14680885720442117314"}},"colab":{"base_uri":"https://localhost:8080/","height":275}},"source":["!pip install pydrive                             # Package to use Google Drive API - not installed in Colab VM by default\n","from pydrive.auth import GoogleAuth\n","from pydrive.drive import GoogleDrive"],"execution_count":3,"outputs":[{"output_type":"stream","text":["Requirement already satisfied: pydrive in /usr/local/lib/python2.7/dist-packages (1.3.1)\n","Requirement already satisfied: PyYAML>=3.0 in /usr/local/lib/python2.7/dist-packages (from pydrive) (3.13)\n","Requirement already satisfied: oauth2client>=4.0.0 in /usr/local/lib/python2.7/dist-packages (from pydrive) (4.1.3)\n","Requirement already satisfied: google-api-python-client>=1.2 in /usr/local/lib/python2.7/dist-packages (from pydrive) (1.7.9)\n","Requirement already satisfied: httplib2>=0.9.1 in /usr/local/lib/python2.7/dist-packages (from oauth2client>=4.0.0->pydrive) (0.11.3)\n","Requirement already satisfied: rsa>=3.1.4 in /usr/local/lib/python2.7/dist-packages (from oauth2client>=4.0.0->pydrive) (4.0)\n","Requirement already satisfied: pyasn1>=0.1.7 in /usr/local/lib/python2.7/dist-packages (from oauth2client>=4.0.0->pydrive) (0.4.5)\n","Requirement already satisfied: pyasn1-modules>=0.0.5 in /usr/local/lib/python2.7/dist-packages (from oauth2client>=4.0.0->pydrive) (0.2.5)\n","Requirement already satisfied: six>=1.6.1 in /usr/local/lib/python2.7/dist-packages (from oauth2client>=4.0.0->pydrive) (1.12.0)\n","Requirement already satisfied: google-auth-httplib2>=0.0.3 in /usr/local/lib/python2.7/dist-packages (from google-api-python-client>=1.2->pydrive) (0.0.3)\n","Requirement already satisfied: google-auth>=1.4.1 in /usr/local/lib/python2.7/dist-packages (from google-api-python-client>=1.2->pydrive) (1.7.2)\n","Requirement already satisfied: uritemplate<4dev,>=3.0.0 in /usr/local/lib/python2.7/dist-packages (from google-api-python-client>=1.2->pydrive) (3.0.0)\n","Requirement already satisfied: setuptools>=40.3.0 in /usr/local/lib/python2.7/dist-packages (from google-auth>=1.4.1->google-api-python-client>=1.2->pydrive) (44.0.0)\n","Requirement already satisfied: cachetools<3.2,>=2.0.0 in /usr/local/lib/python2.7/dist-packages (from google-auth>=1.4.1->google-api-python-client>=1.2->pydrive) (3.1.1)\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"axY-zgFs02wQ","colab_type":"code","outputId":"4eac73c4-78fa-4438-ab57-6f6e485c6660","executionInfo":{"status":"ok","timestamp":1585771418656,"user_tz":-330,"elapsed":7630,"user":{"displayName":"Uttam Deshpande","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gi0ntRxzuR6q-uoz9PehErcklTqufxtYeGtc1T69w=s64","userId":"14680885720442117314"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["from google.colab import drive\n","drive.mount('/content/drive')\n","#drive.mount(\"/content/drive\", force_remount=True)"],"execution_count":4,"outputs":[{"output_type":"stream","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"cu6cjK0eQv6x","colab_type":"code","colab":{}},"source":["os.chdir('drive/My Drive/LearnNet/CoarseNet/')"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"FrNApdjtuIEP","colab_type":"code","outputId":"93a875a2-8cad-4ac0-8692-8436c9d94d85","executionInfo":{"status":"ok","timestamp":1585771418658,"user_tz":-330,"elapsed":7606,"user":{"displayName":"Uttam Deshpande","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gi0ntRxzuR6q-uoz9PehErcklTqufxtYeGtc1T69w=s64","userId":"14680885720442117314"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["#\n","#os.chdir('drive/My Drive/Colab Notebooks/LearnNet/CoarseNet')\n","os.getcwd()\n","\n"],"execution_count":6,"outputs":[{"output_type":"execute_result","data":{"text/plain":["'/content/drive/My Drive/LearnNet/CoarseNet'"]},"metadata":{"tags":[]},"execution_count":6}]},{"cell_type":"code","metadata":{"id":"dx_xHOhXUTcW","colab_type":"code","colab":{}},"source":["from google.colab import auth                    # Other necessary packages\n","from oauth2client.client import GoogleCredentials\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"F5R10_VHV_A3","colab_type":"code","colab":{}},"source":["auth.authenticate_user()                         # Follow prompt in the authorization process\n","gauth = GoogleAuth()\n","gauth.credentials = GoogleCredentials.get_application_default()"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"9CIMKFy9d63X","colab_type":"code","colab":{}},"source":["drive = GoogleDrive(gauth)\n","your_module = drive.CreateFile({\"id\": \"1RM5LRWpO3bGRitnOl0uvF_8aqb798wHf\"})   # \"your_module_file_id\" is the part after \"id=\" in the shareable link\n","your_module.GetContentFile(\"CNAI.py\")          # Save the .py module file to Colab VM\n","import CNAI  "],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"Bc0PheYlpYV2","colab_type":"code","outputId":"acca954d-c1db-44ab-c122-57aa58b29b09","executionInfo":{"status":"ok","timestamp":1585771423610,"user_tz":-330,"elapsed":12515,"user":{"displayName":"Uttam Deshpande","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gi0ntRxzuR6q-uoz9PehErcklTqufxtYeGtc1T69w=s64","userId":"14680885720442117314"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["pip install XlsxWriter"],"execution_count":10,"outputs":[{"output_type":"stream","text":["Requirement already satisfied: XlsxWriter in /usr/local/lib/python2.7/dist-packages (1.2.8)\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"e93ps5kwjrbO","colab_type":"code","colab":{}},"source":["#!pip install q keras==2.2.4"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"EPwQcArIbGmL","colab_type":"code","outputId":"99a5bba6-1515-4e29-a410-10702efe11a0","executionInfo":{"status":"ok","timestamp":1585771424525,"user_tz":-330,"elapsed":13405,"user":{"displayName":"Uttam Deshpande","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gi0ntRxzuR6q-uoz9PehErcklTqufxtYeGtc1T69w=s64","userId":"14680885720442117314"}},"colab":{"base_uri":"https://localhost:8080/","height":97}},"source":["import keras\n","keras.__version__"],"execution_count":12,"outputs":[{"output_type":"stream","text":["Using TensorFlow backend.\n"],"name":"stderr"},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["<p style=\"color: red;\">\n","The default version of TensorFlow in Colab will switch to TensorFlow 2.x on the 27th of March, 2020.<br>\n","We recommend you <a href=\"https://www.tensorflow.org/guide/migrate\" target=\"_blank\">upgrade</a> now\n","or ensure your notebook will continue to use TensorFlow 1.x via the <code>%tensorflow_version 1.x</code> magic:\n","<a href=\"https://colab.research.google.com/notebooks/tensorflow_version.ipynb\" target=\"_blank\">more info</a>.</p>\n"]},"metadata":{"tags":[]}},{"output_type":"execute_result","data":{"text/plain":["'2.2.4'"]},"metadata":{"tags":[]},"execution_count":12}]},{"cell_type":"code","metadata":{"id":"NL6BfLuhbl0P","colab_type":"code","outputId":"a858f3d6-9349-45eb-9882-eef874dfad64","executionInfo":{"status":"ok","timestamp":1585771427618,"user_tz":-330,"elapsed":16483,"user":{"displayName":"Uttam Deshpande","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gi0ntRxzuR6q-uoz9PehErcklTqufxtYeGtc1T69w=s64","userId":"14680885720442117314"}},"colab":{"base_uri":"https://localhost:8080/","height":493}},"source":["!pip install tensorflow==1.15\n"],"execution_count":13,"outputs":[{"output_type":"stream","text":["Requirement already satisfied: tensorflow==1.15 in /usr/local/lib/python2.7/dist-packages (1.15.0)\n","Requirement already satisfied: gast==0.2.2 in /usr/local/lib/python2.7/dist-packages (from tensorflow==1.15) (0.2.2)\n","Requirement already satisfied: grpcio>=1.8.6 in /usr/local/lib/python2.7/dist-packages (from tensorflow==1.15) (1.15.0)\n","Requirement already satisfied: numpy<2.0,>=1.16.0 in /usr/local/lib/python2.7/dist-packages (from tensorflow==1.15) (1.16.4)\n","Requirement already satisfied: backports.weakref>=1.0rc1; python_version < \"3.4\" in /usr/local/lib/python2.7/dist-packages (from tensorflow==1.15) (1.0.post1)\n","Requirement already satisfied: functools32>=3.2.3 in /usr/local/lib/python2.7/dist-packages (from tensorflow==1.15) (3.2.3.post2)\n","Requirement already satisfied: tensorboard<1.16.0,>=1.15.0 in /usr/local/lib/python2.7/dist-packages (from tensorflow==1.15) (1.15.0)\n","Requirement already satisfied: protobuf>=3.6.1 in /usr/local/lib/python2.7/dist-packages (from tensorflow==1.15) (3.8.0)\n","Requirement already satisfied: keras-preprocessing>=1.0.5 in /usr/local/lib/python2.7/dist-packages (from tensorflow==1.15) (1.1.0)\n","Requirement already satisfied: mock>=2.0.0 in /usr/local/lib/python2.7/dist-packages (from tensorflow==1.15) (2.0.0)\n","Requirement already satisfied: tensorflow-estimator==1.15.1 in /usr/local/lib/python2.7/dist-packages (from tensorflow==1.15) (1.15.1)\n","Requirement already satisfied: keras-applications>=1.0.8 in /usr/local/lib/python2.7/dist-packages (from tensorflow==1.15) (1.0.8)\n","Requirement already satisfied: wheel in /usr/local/lib/python2.7/dist-packages (from tensorflow==1.15) (0.34.2)\n","Requirement already satisfied: wrapt>=1.11.1 in /usr/local/lib/python2.7/dist-packages (from tensorflow==1.15) (1.11.2)\n","Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python2.7/dist-packages (from tensorflow==1.15) (1.12.0)\n","Requirement already satisfied: absl-py>=0.7.0 in /usr/local/lib/python2.7/dist-packages (from tensorflow==1.15) (0.7.1)\n","Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python2.7/dist-packages (from tensorflow==1.15) (2.3.2)\n","Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python2.7/dist-packages (from tensorflow==1.15) (1.1.0)\n","Requirement already satisfied: google-pasta>=0.1.6 in /usr/local/lib/python2.7/dist-packages (from tensorflow==1.15) (0.1.7)\n","Requirement already satisfied: enum34>=1.1.6; python_version < \"3.4\" in /usr/local/lib/python2.7/dist-packages (from tensorflow==1.15) (1.1.6)\n","Requirement already satisfied: astor>=0.6.0 in /usr/local/lib/python2.7/dist-packages (from tensorflow==1.15) (0.8.0)\n","Requirement already satisfied: futures>=2.2.0 in /usr/local/lib/python2.7/dist-packages (from grpcio>=1.8.6->tensorflow==1.15) (3.2.0)\n","Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python2.7/dist-packages (from tensorboard<1.16.0,>=1.15.0->tensorflow==1.15) (44.0.0)\n","Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python2.7/dist-packages (from tensorboard<1.16.0,>=1.15.0->tensorflow==1.15) (0.15.5)\n","Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python2.7/dist-packages (from tensorboard<1.16.0,>=1.15.0->tensorflow==1.15) (3.1.1)\n","Requirement already satisfied: funcsigs>=1; python_version < \"3.3\" in /usr/local/lib/python2.7/dist-packages (from mock>=2.0.0->tensorflow==1.15) (1.0.2)\n","Requirement already satisfied: pbr>=0.11 in /usr/local/lib/python2.7/dist-packages (from mock>=2.0.0->tensorflow==1.15) (5.4.0)\n","Requirement already satisfied: h5py in /usr/local/lib/python2.7/dist-packages (from keras-applications>=1.0.8->tensorflow==1.15) (2.8.0)\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"ZU6rQTgFGi0W","colab_type":"code","outputId":"0323e7c0-c645-40ee-d1b8-69d8cd2e58d6","executionInfo":{"status":"ok","timestamp":1585771524161,"user_tz":-330,"elapsed":113011,"user":{"displayName":"Uttam Deshpande","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gi0ntRxzuR6q-uoz9PehErcklTqufxtYeGtc1T69w=s64","userId":"14680885720442117314"}},"colab":{"base_uri":"https://localhost:8080/","height":1000}},"source":["from __future__ import absolute_import\n","from __future__ import division\n","import os\n","os.environ['KERAS_BACKEND'] = 'tensorflow'\n","from datetime import datetime\n","import matplotlib.pyplot as plt\n","import numpy as np\n","from scipy import ndimage, misc, signal, spatial,sparse,io\n","from skimage.filters import gaussian\n","import cv2\n","import math\n","from time import time\n","from time import sleep\n","import sys\n","from keras.models import Model\n","from keras.layers import Input,Concatenate,GlobalAveragePooling2D\n","from keras import layers\n","from keras.layers.core import Flatten,Activation,Lambda, Dropout\n","from keras.layers.convolutional import Conv2D,MaxPooling2D,UpSampling2D,AveragePooling2D\n","from keras.layers.normalization import BatchNormalization\n","from keras.layers.advanced_activations import PReLU\n","from keras.regularizers import l2\n","from keras.optimizers import SGD, Adam\n","from keras.utils import plot_model\n","import tensorflow as tf\n","from keras_applications.imagenet_utils import _obtain_input_shape\n","from keras import backend as K\n","from functools import reduce\n","import glob\n","import logging\n","import json\n","import CNAI\n","import shutil\n","from keras.models import Sequential\n","from keras.models import load_model\n","from keras.layers import Dense\n","from keras.optimizers import Adadelta\n","from keras.layers.convolutional import Conv1D\n","from keras.layers.convolutional import MaxPooling1D\n","from keras.utils import to_categorical\n","import collections\n","from collections import Counter\n","import xlsxwriter\n","\n","\n","\n","os.environ[\"CUDA_VISIBLE_DEVICES\"] = '0'\n","\n","config = K.tf.ConfigProto(gpu_options=K.tf.GPUOptions(allow_growth=True))\n","sess = K.tf.Session(config=config)\n","K.set_session(sess)\n","\n","mode = 'inference'\n","root=os.getcwd()\n","# Can use multiple folders for deploy, inference\n","inference_set = ['../Dataset/CoarseNet_test/',]\n","\n","pretrain_dir = '../Models/CoarseNet.h5'\n","cnai_model_dir='../Models/cnai_weight.h5'\n","output_dir = '../output_CoarseNet/'+datetime.now().strftime('%Y%m%d-%H%M%S')\n","\n","FineNet_dir = '../Models/FineNet.h5'\n","\n","def main():\n","    if mode == 'inference':\n","       # output_dir = '../output_CoarseNet/inferenceResults/' +datetime.now().strftime('%Y%m%d-%H%M%S')\n","        logging = init_log(output_dir)\n","        for i, folder in enumerate(inference_set):\n","            inference(folder, output_dir=output_dir, model_path=pretrain_dir, FineNet_path=FineNet_dir, file_ext='.bmp',\n","                      isHavingFineNet=False)\n","    else:\n","        pass\n","\n","def init_log(output_dir):\n","    re_mkdir(output_dir)\n","    logging.basicConfig(level=logging.DEBUG,\n","        format='%(asctime)s %(message)s',\n","        datefmt='%Y%m%d-%H:%M:%S',\n","        filename=os.path.join(output_dir, 'log.log'),\n","        filemode='w')\n","    console = logging.StreamHandler()\n","    console.setLevel(logging.INFO)\n","    logging.getLogger('').addHandler(console)\n","    return logging\n","\n","def re_mkdir(path):\n","    if os.path.exists(path):\n","        shutil.rmtree(path)\n","    os.makedirs(path)\n","\n","def mkdir(path):\n","    if not os.path.exists(path):\n","        os.makedirs(path)\n","        \n","\n","def get_files_in_folder(folder, file_ext=None):\n","    files = glob.glob(os.path.join(folder, \"*\" + file_ext))\n","    files_name = []\n","    for i in files:\n","        _, name = os.path.split(i)\n","        name, ext = os.path.splitext(name)\n","        files_name.append(name)\n","    return np.asarray(files), np.asarray(files_name)\n","def nextpow2(x):\n","    return int(math.ceil(math.log(x, 2)))\n","def smooth_dir_map(dir_map,sigma=2.0,mask = None):\n","\n","    cos2Theta = np.cos(dir_map * 2)\n","    sin2Theta = np.sin(dir_map * 2)\n","    if mask is not None:\n","        assert (dir_map.shape[0] == mask.shape[0])\n","        assert (dir_map.shape[1] == mask.shape[1])\n","        cos2Theta[mask == 0] = 0\n","        sin2Theta[mask == 0] = 0\n","\n","    cos2Theta = gaussian(cos2Theta, sigma, multichannel=False, mode='reflect')\n","    sin2Theta = gaussian(sin2Theta, sigma, multichannel=False, mode='reflect')\n","\n","    dir_map = np.arctan2(sin2Theta,cos2Theta)*0.5\n","\n","\n","    return dir_map\n","\n","def compute_gradient_norm(input):\n","    input = input.astype(np.float32)\n","\n","    Gx, Gy = np.gradient(input)\n","    out = np.sqrt(Gx * Gx + Gy * Gy) + 0.000001\n","    return out\n","def get_ridge_flow_top(local_info):\n","\n","    blkH,blkW = local_info.shape\n","    dir_map = np.zeros((blkH,blkW)) - 10\n","    fre_map = np.zeros((blkH, blkW)) - 10\n","    for i in range(blkH):\n","        for j in range(blkW):\n","            if local_info[i,j].ori is None:\n","                continue\n","\n","            dir_map[i,j] = local_info[i,j].ori[0] #+ math.pi*0.5\n","            fre_map[i,j] = local_info[i,j].fre[0]\n","    return dir_map,fre_map\n","\n","def inference(deploy_set, output_dir, model_path, FineNet_path=None, set_name=None, file_ext='.bmp', isHavingFineNet = False):\n","    if set_name is None:\n","        set_name = deploy_set.split('/')[-2]\n","\n","\n","    mkdir(output_dir + '/'+ set_name + '/')\n","    mkdir(output_dir + '/' + set_name + '/mnt_results/')\n","    mkdir(output_dir + '/'+ set_name + '/seg_results/')\n","    mkdir(output_dir + '/' + set_name + '/OF_results/')\n","\n","    logging.info(\"Predicting %s:\" % (set_name))\n","\n","    _, img_name = get_files_in_folder(deploy_set+ 'img_files/', file_ext)\n","    print(deploy_set)\n","\n","    time_c = []\n","\n","    main_net_model = CoarseNetmodel((None, None, 1), model_path, mode='inference')\n","\n","    for i in range(0, len(img_name)):\n","        print(i)\n","\n","        image = misc.imread(deploy_set + 'img_files/'+ img_name[i] + file_ext, mode='L')  # / 255.0\n","\n","        img_size = image.shape\n","        img_size = np.array(img_size, dtype=np.int32) // 8 * 8\n","\n","        # read the mask from files\n","        try:\n","            mask = misc.imread(deploy_set + 'seg_files/' + img_name[i] + '.jpg', mode='L') / 255.0\n","        except:\n","            mask = np.ones((img_size[0],img_size[1]))\n","        image = image[:img_size[0], :img_size[1]]\n","        mask = mask[:img_size[0], :img_size[1]]\n","\n","        original_image = image.copy()\n","\n","        texture_img = FastEnhanceTexture(image, sigma=2.5, show=False)\n","        dir_map, fre_map = get_maps_STFT(texture_img, patch_size=64, block_size=16, preprocess=True)\n","        image = image*mask\n","\n","        logging.info(\"%s %d / %d: %s\" % (set_name, i + 1, len(img_name), img_name[i]))\n","        time_start = time()\n","\n","        image = np.reshape(image, [1, image.shape[0], image.shape[1], 1])\n","\n","        enh_img, enh_img_imag, enhance_img, ori_out_1, ori_out_2, seg_out, mnt_o_out, mnt_w_out, mnt_h_out, mnt_s_out \\\n","            = main_net_model.predict(image)\n","        time_afterconv = time()\n","\n","        # If use mask from model\n","        round_seg = np.round(np.squeeze(seg_out))\n","        seg_out = 1 - round_seg\n","        kernel = cv2.getStructuringElement(cv2.MORPH_RECT, (10, 10))\n","        seg_out = cv2.morphologyEx(seg_out, cv2.MORPH_CLOSE, kernel)\n","        kernel = cv2.getStructuringElement(cv2.MORPH_RECT, (7, 7))\n","        seg_out = cv2.morphologyEx(seg_out, cv2.MORPH_OPEN, kernel)\n","        kernel = cv2.getStructuringElement(cv2.MORPH_RECT, (5, 5))\n","        seg_out = cv2.dilate(seg_out, kernel)\n","        # If use mask from outside\n","        max_num_minu = 20\n","        min_num_minu = 6\n","\n","        #early_minutiae_thres = 0.5\n","        early_minutiae_thres = 0.1\n","        # New adaptive threshold\n","        mnt = label2mnt(np.squeeze(mnt_s_out) * np.round(np.squeeze(seg_out)), mnt_w_out, mnt_h_out, mnt_o_out,\n","                        thresh=0)\n","\n","        # Previous exp: 0.2\n","        mnt_nms_1 = py_cpu_nms(mnt, 0.1)\n","        mnt_nms_2 = nms(mnt)\n","        mnt_nms_1.view('f8,f8,f8,f8').sort(order=['f3'], axis=0)\n","        mnt_nms_1 = mnt_nms_1[::-1]\n","\n","        mnt_nms_1_copy = mnt_nms_1.copy()\n","        mnt_nms_2_copy = mnt_nms_2.copy()\n","        # Adaptive threshold goes here\n","        # Make sure the maximum number of minutiae is max_num_minu\n","\n","        # Sort minutiae by score\n","        while early_minutiae_thres > 0:\n","            mnt_nms_1 = mnt_nms_1_copy[mnt_nms_1_copy[:, 3] > early_minutiae_thres, :]\n","            mnt_nms_2 = mnt_nms_2_copy[mnt_nms_2_copy[:, 3] > early_minutiae_thres, :]\n","\n","            if mnt_nms_1.shape[0]>max_num_minu or mnt_nms_2.shape[0]>max_num_minu:\n","                mnt_nms_1 = mnt_nms_1[:max_num_minu,:]\n","                mnt_nms_2 = mnt_nms_2[:max_num_minu, :]\n","            if mnt_nms_1.shape[0] > min_num_minu and mnt_nms_2.shape[0] > min_num_minu:\n","                break\n","\n","            early_minutiae_thres = early_minutiae_thres - 0.05\n","\n","        mnt_nms = fuse_nms(mnt_nms_1, mnt_nms_2)\n","\n","        final_minutiae_score_threashold = early_minutiae_thres - 0.05\n","\n","        print(early_minutiae_thres, final_minutiae_score_threashold)\n","\n","        mnt_nms_backup = mnt_nms.copy()\n","        mnt_nms = np.array(mnt_nms)\n","\n","        if mnt_nms.shape[0] > 0:\n","            mnt_nms = mnt_nms[mnt_nms[:,3]>final_minutiae_score_threashold,:]\n","\n","        final_mask = ndimage.zoom(np.round(np.squeeze(seg_out)), [8, 8], order=0)\n","        # Show the orientation\n","        show_orientation_field(original_image, dir_map + np.pi, mask=final_mask, fname=\"%s/%s/OF_results/%s_OF.jpg\" % (output_dir, set_name, img_name[i]))\n","\n","        fuse_minu_orientation(dir_map, mnt_nms, mode=3)\n","\n","        time_afterpost = time()\n","        mnt_writer(mnt_nms, img_name[i], img_size, \"%s/%s/mnt_results/%s.mnt\"%(output_dir, set_name, img_name[i]))\n","        draw_minutiae(original_image, mnt_nms, \"%s/%s/%s_minu.jpg\"%(output_dir, set_name, img_name[i]),saveimage=True)\n","\n","        misc.imsave(\"%s/%s/seg_results/%s_seg.jpg\" % (output_dir, set_name, img_name[i]), final_mask)\n","\n","        time_afterdraw = time()\n","        time_c.append([time_afterconv - time_start, time_afterpost - time_afterconv, time_afterdraw - time_afterpost])\n","        logging.info(\n","            \"load+conv: %.3fs, seg-postpro+nms: %.3f, draw: %.3f\" % (time_c[-1][0], time_c[-1][1], time_c[-1][2]))\n","    return\n","\n","def draw_minutiae(image, minutiae, fname, saveimage= False, r=15, drawScore=True):\n","    image = np.squeeze(image)\n","    fig = plt.figure()\n","    \n","\n","    plt.imshow(image,cmap='gray')\n","    \n","    # Check if no minutiae\n","    if minutiae.shape[0] > 0:\n","        plt.plot(minutiae[:, 0], minutiae[:, 1], 'rs', fillstyle='none', linewidth=1)\n","        for x, y, o, s in minutiae:\n","            #plt.plot([x, x+r*np.cos(o)], [y, y+r*np.sin(o)], 'r-')\n","            if drawScore == True:\n","                plt.text(x - 10, y - 10, '%.2f' % s, color='yellow', fontsize=4)\n","\n","    plt.axis([0,image.shape[1],image.shape[0],0])\n","    plt.axis('off')\n","    if saveimage:\n","        plt.savefig(fname, dpi=500, bbox_inches='tight', pad_inches = 0)\n","        plt.close(fig)\n","    else:\n","        plt.show()\n","    return\n","def mnt_writer(mnt, image_name, image_size, file_name):\n","    f = open(file_name, 'w')\n","    f.write('%s\\n'%(image_name))\n","    f.write('%d %d %d\\n'%(mnt.shape[0], image_size[0], image_size[1]))\n","    for i in range(mnt.shape[0]):\n","        f.write('%d %d %.6f %.4f\\n'%(mnt[i,0], mnt[i,1], mnt[i,2], mnt[i,3]))\n","    f.close()\n","    return\n","def copy_file(path_s, path_t):\n","    shutil.copy(path_s, path_t)  \n","def conv_bn_prelu(bottom, w_size, name, strides=(1,1), dilation_rate=(1,1)):\n","    if dilation_rate == (1,1):\n","        conv_type = 'conv'\n","    else:\n","        conv_type = 'atrousconv'\n","\n","    top = Conv2D(w_size[0], (w_size[1],w_size[2]),\n","        kernel_regularizer=l2(5e-5),\n","        padding='same',\n","        strides=strides,\n","        dilation_rate=dilation_rate,\n","        name=conv_type+name)(bottom)\n","    top = BatchNormalization(name='bn-'+name)(top)\n","    top = PReLU(alpha_initializer='zero', shared_axes=[1,2], name='prelu-'+name)(top)\n","    # top = Dropout(0.25)(top)\n","    return top\n","\n","def CoarseNetmodel(input_shape=(400,400,1), weights_path=None, mode='train'):\n","    # Change network architecture here!!\n","    img_input=Input(input_shape)\n","    bn_img=Lambda(img_normalization, name='img_normalized')(img_input)\n","\n","    # Main part\n","    conv = conv_bn_prelu(bn_img, (64, 5, 5), '1_0')\n","    conv = conv_bn_prelu(conv, (64, 3, 3), '1_1')\n","    conv = conv_bn_prelu(conv, (64, 3, 3), '1_2')\n","    conv = MaxPooling2D(pool_size=(2, 2), strides=(2, 2))(conv)\n","\n","    # =======Block 1 ========\n","    conv1 = conv_bn_prelu(conv, (128, 3, 3), '2_1')\n","    conv = conv_bn_prelu(conv1, (128, 3, 3), '2_2')\n","    conv = conv_bn_prelu(conv, (128, 3, 3), '2_3')\n","    conv = layers.add([conv, conv1])\n","\n","    conv1 = conv_bn_prelu(conv, (128, 3, 3), '2_1b')\n","    conv = conv_bn_prelu(conv1, (128, 3, 3), '2_2b')\n","    conv = conv_bn_prelu(conv, (128, 3, 3), '2_3b')\n","    conv = layers.add([conv, conv1])\n","\n","    conv1 = conv_bn_prelu(conv, (128, 3, 3), '2_1c')\n","    conv = conv_bn_prelu(conv1, (128, 3, 3), '2_2c')\n","    conv = conv_bn_prelu(conv, (128, 3, 3), '2_3c')\n","    conv = layers.add([conv, conv1])\n","\n","    conv_block1 = MaxPooling2D(pool_size=(2,2),strides=(2,2))(conv)\n","    # ==========================\n","\n","    # =======Block 2 ========\n","    conv1 = conv_bn_prelu(conv_block1, (256,3,3), '3_1')\n","    conv = conv_bn_prelu(conv1, (256,3,3), '3_2')\n","    conv = conv_bn_prelu(conv, (256,3,3), '3_3')\n","    conv = layers.add([conv, conv1])\n","\n","    conv1 = conv_bn_prelu(conv, (256, 3, 3), '3_1b')\n","    conv = conv_bn_prelu(conv1, (256, 3, 3), '3_2b')\n","    conv = conv_bn_prelu(conv, (256, 3, 3), '3_3b')\n","    conv = layers.add([conv, conv1])\n","\n","    conv_block2 = MaxPooling2D(pool_size=(2,2),strides=(2,2))(conv)\n","    # ==========================\n","\n","    # =======Block 3 ========\n","    conv1 = conv_bn_prelu(conv_block2, (512, 3, 3), '3_1c')\n","    conv = conv_bn_prelu(conv1, (512, 3, 3), '3_2c')\n","    conv = conv_bn_prelu(conv, (512, 3, 3), '3_3c')\n","    conv = layers.add([conv, conv1])\n","    conv_block3 = conv_bn_prelu(conv, (256, 3, 3), '3_4c')\n","\n","    #conv_block3 = MaxPooling2D(pool_size=(2,2),strides=(2,2))(conv)\n","    # ==========================\n","\n","\n","    # multi-scale ASPP\n","    level_2=conv_bn_prelu(conv_block3, (256,3,3), '4_1', dilation_rate=(1,1))\n","    ori_1=conv_bn_prelu(level_2, (128,1,1), 'ori_1_1')\n","    ori_1=Conv2D(90, (1,1), padding='same', name='ori_1_2')(ori_1)\n","    seg_1=conv_bn_prelu(level_2, (128,1,1), 'seg_1_1')\n","    seg_1=Conv2D(1, (1,1), padding='same', name='seg_1_2')(seg_1)\n","\n","    level_3=conv_bn_prelu(conv_block2, (256,3,3), '4_2', dilation_rate=(4,4))\n","    ori_2=conv_bn_prelu(level_3, (128,1,1), 'ori_2_1')\n","    ori_2=Conv2D(90, (1,1), padding='same', name='ori_2_2')(ori_2)\n","    seg_2=conv_bn_prelu(level_3, (128,1,1), 'seg_2_1')\n","    seg_2=Conv2D(1, (1,1), padding='same', name='seg_2_2')(seg_2)\n","\n","    level_4=conv_bn_prelu(conv_block2, (256,3,3), '4_3', dilation_rate=(8,8))\n","    ori_3=conv_bn_prelu(level_4, (128,1,1), 'ori_3_1')\n","    ori_3=Conv2D(90, (1,1), padding='same', name='ori_3_2')(ori_3)\n","    seg_3=conv_bn_prelu(level_4, (128,1,1), 'seg_3_1')\n","    seg_3=Conv2D(1, (1,1), padding='same', name='seg_3_2')(seg_3)\n","\n","    # sum fusion for ori\n","    ori_out=Lambda(merge_sum)([ori_1, ori_2, ori_3])\n","    ori_out_1=Activation('sigmoid', name='ori_out_1')(ori_out)\n","    ori_out_2=Activation('sigmoid', name='ori_out_2')(ori_out)\n","\n","    # sum fusion for segmentation\n","    seg_out=Lambda(merge_sum)([seg_1, seg_2, seg_3])\n","    seg_out=Activation('sigmoid', name='seg_out')(seg_out)\n","\n","    # ----------------------------------------------------------------------------\n","    # enhance part\n","    filters_cos, filters_sin = gabor_bank(stride=2, Lambda=8)\n","\n","    filter_img_real = Conv2D(filters_cos.shape[3],(filters_cos.shape[0],filters_cos.shape[1]),\n","        weights=[filters_cos, np.zeros([filters_cos.shape[3]])], padding='same',\n","        name='enh_img_real_1')(img_input)\n","    filter_img_imag = Conv2D(filters_sin.shape[3],(filters_sin.shape[0],filters_sin.shape[1]),\n","        weights=[filters_sin, np.zeros([filters_sin.shape[3]])], padding='same',\n","        name='enh_img_imag_1')(img_input)\n","\n","    ori_peak = Lambda(ori_highest_peak)(ori_out_1)\n","    ori_peak = Lambda(select_max)(ori_peak) # select max ori and set it to 1\n","\n","    # Use this function to upsample image\n","    upsample_ori = UpSampling2D(size=(8,8))(ori_peak)\n","    seg_round = Activation('softsign')(seg_out)\n","\n","\n","    upsample_seg = UpSampling2D(size=(8,8))(seg_round)\n","    mul_mask_real = Lambda(merge_mul)([filter_img_real, upsample_ori])\n","\n","    enh_img_real = Lambda(reduce_sum, name='enh_img_real_2')(mul_mask_real)\n","    mul_mask_imag = Lambda(merge_mul)([filter_img_imag, upsample_ori])\n","\n","    enh_img_imag = Lambda(reduce_sum, name='enh_img_imag_2')(mul_mask_imag)\n","    enh_img = Lambda(atan2, name='phase_img')([enh_img_imag, enh_img_real])\n","\n","    enhance_img = Lambda(merge_concat, name='phase_seg_img')([enh_img, upsample_seg])\n","    # ----------------------------------------------------------------------------\n","    # mnt part\n","    # =======Block 1 ========\n","    mnt_conv1 = conv_bn_prelu(enhance_img, (64, 9, 9), 'mnt_1_1')\n","    mnt_conv = conv_bn_prelu(mnt_conv1, (64, 9, 9), 'mnt_1_2')\n","    mnt_conv = conv_bn_prelu(mnt_conv, (64, 9, 9), 'mnt_1_3')\n","    mnt_conv = layers.add([mnt_conv, mnt_conv1])\n","\n","    mnt_conv1 = conv_bn_prelu(mnt_conv, (64, 9, 9), 'mnt_1_1b')\n","    mnt_conv = conv_bn_prelu(mnt_conv1, (64, 9, 9), 'mnt_1_2b')\n","    mnt_conv = conv_bn_prelu(mnt_conv, (64, 9, 9), 'mnt_1_3b')\n","    mnt_conv = layers.add([mnt_conv, mnt_conv1])\n","\n","    mnt_conv = MaxPooling2D(pool_size=(2, 2), strides=(2, 2))(mnt_conv)\n","    # ==========================\n","\n","    # =======Block 2 ========\n","    mnt_conv1 = conv_bn_prelu(mnt_conv, (128, 5, 5), 'mnt_2_1')\n","    mnt_conv = conv_bn_prelu(mnt_conv1, (128, 5, 5), 'mnt_2_2')\n","    mnt_conv = conv_bn_prelu(mnt_conv, (128, 5, 5), 'mnt_2_3')\n","    mnt_conv = layers.add([mnt_conv, mnt_conv1])\n","\n","    mnt_conv1 = conv_bn_prelu(mnt_conv, (128, 5, 5), 'mnt_2_1b')\n","    mnt_conv = conv_bn_prelu(mnt_conv1, (128, 5, 5), 'mnt_2_2b')\n","    mnt_conv = conv_bn_prelu(mnt_conv, (128, 5, 5), 'mnt_2_3b')\n","    mnt_conv = layers.add([mnt_conv, mnt_conv1])\n","\n","    mnt_conv = MaxPooling2D(pool_size=(2, 2), strides=(2, 2))(mnt_conv)\n","    # ==========================\n","\n","    # =======Block 3 ========\n","    mnt_conv1 = conv_bn_prelu(mnt_conv, (256, 3, 3), 'mnt_3_1')\n","    mnt_conv2 = conv_bn_prelu(mnt_conv1, (256, 3, 3), 'mnt_3_2')\n","    mnt_conv3 = conv_bn_prelu(mnt_conv2, (256, 3, 3), 'mnt_3_3')\n","    mnt_conv3 = layers.add([mnt_conv3, mnt_conv1])\n","    mnt_conv4 = conv_bn_prelu(mnt_conv3, (256, 3, 3), 'mnt_3_4')\n","    mnt_conv4 = layers.add([mnt_conv4, mnt_conv2])\n","\n","    mnt_conv = MaxPooling2D(pool_size=(2, 2), strides=(2, 2))(mnt_conv4)\n","    # ==========================\n","\n","\n","    mnt_o_1=Lambda(merge_concat)([mnt_conv, ori_out_1])\n","    mnt_o_2=conv_bn_prelu(mnt_o_1, (256,1,1), 'mnt_o_1_1')\n","    mnt_o_3=Conv2D(180, (1,1), padding='same', name='mnt_o_1_2')(mnt_o_2)\n","    mnt_o_out=Activation('sigmoid', name='mnt_o_out')(mnt_o_3)\n","\n","    mnt_w_1=conv_bn_prelu(mnt_conv, (256,1,1), 'mnt_w_1_1')\n","    mnt_w_2=Conv2D(8, (1,1), padding='same', name='mnt_w_1_2')(mnt_w_1)\n","    mnt_w_out=Activation('sigmoid', name='mnt_w_out')(mnt_w_2)\n","\n","    mnt_h_1=conv_bn_prelu(mnt_conv, (256,1,1), 'mnt_h_1_1')\n","    mnt_h_2=Conv2D(8, (1,1), padding='same', name='mnt_h_1_2')(mnt_h_1)\n","    mnt_h_out=Activation('sigmoid', name='mnt_h_out')(mnt_h_2)\n","\n","    mnt_s_1=conv_bn_prelu(mnt_conv, (256,1,1), 'mnt_s_1_1')\n","    mnt_s_2=Conv2D(1, (1,1), padding='same', name='mnt_s_1_2')(mnt_s_1)\n","    mnt_s_out=Activation('sigmoid', name='mnt_s_out')(mnt_s_2)\n","\n","      \n","    model = Model(inputs=[img_input,], outputs=[enh_img,enh_img_imag,enhance_img,ori_out_1, ori_out_2, seg_out, mnt_o_out, mnt_w_out, mnt_h_out, mnt_s_out])\n","\n","    if weights_path != None:\n","        model.load_weights(weights_path, by_name=True)\n","    return model\n","def LowpassFiltering(img,L):\n","    h,w = img.shape\n","    h2,w2 = L.shape\n","\n","    img = cv2.copyMakeBorder(img, 0, h2-h, 0, w2-w, cv2.BORDER_CONSTANT, value=0)\n","\n","    img_fft = np.fft.fft2(img)\n","    img_fft = np.fft.fftshift(img_fft)\n","\n","    img_fft = img_fft * L\n","    rec_img = np.fft.ifft2(np.fft.fftshift(img_fft))\n","    rec_img = np.real(rec_img)\n","    rec_img = rec_img[:h,:w]\n","\n","    return rec_img\n","\n","def FastEnhanceTexture(img,sigma=2.5,show=False):\n","    img = img.astype(np.float32)\n","    h, w = img.shape\n","    h2 = 2 ** nextpow2(h)\n","    w2 = 2 ** nextpow2(w)\n","\n","    FFTsize = np.max([h2, w2])\n","    x, y = np.meshgrid(range(int(-FFTsize / 2), int(FFTsize / 2)), range(int(-FFTsize / 2), int(FFTsize / 2)))\n","    r = np.sqrt(x * x + y * y) + 0.0001\n","    r = r/FFTsize\n","\n","    L = 1. / (1 + (2 * math.pi * r * sigma)** 4)\n","    img_low = LowpassFiltering(img, L)\n","\n","    gradim1=  compute_gradient_norm(img)\n","    gradim1 = LowpassFiltering(gradim1,L)\n","\n","    gradim2=  compute_gradient_norm(img_low)\n","    gradim2 = LowpassFiltering(gradim2,L)\n","\n","    diff = gradim1-gradim2\n","    ar1 = np.abs(gradim1)\n","    diff[ar1>1] = diff[ar1>1]/ar1[ar1>1]\n","    diff[ar1 <= 1] = 0\n","\n","    cmin = 0.3\n","    cmax = 0.7\n","\n","    weight = (diff-cmin)/(cmax-cmin)\n","    weight[diff<cmin] = 0\n","    weight[diff>cmax] = 1\n","\n","    u = weight * img_low + (1-weight)* img\n","    temp = img - u\n","    lim = 20\n","    temp1 = (temp + lim) * 255 / (2 * lim)\n","    temp1[temp1 < 0] = 0\n","    temp1[temp1 >255] = 255\n","    v = temp1\n","    if show:\n","        plt.imshow(v,cmap='gray')\n","        plt.show()\n","    return v\n","\n","def get_maps_STFT(img,patch_size = 64,block_size = 16, preprocess = False):\n","    assert len(img.shape) == 2\n","\n","    nrof_dirs = 16\n","    ovp_size = (patch_size-block_size)//2\n","    if preprocess:\n","        img = FastEnhanceTexture(img, sigma=2.5, show=False)\n","\n","    img = np.lib.pad(img, (ovp_size,ovp_size),'symmetric')\n","    h,w = img.shape\n","    blkH = (h - patch_size)//block_size+1\n","    blkW = (w - patch_size)//block_size+1\n","    local_info = np.empty((blkH,blkW),dtype = object)\n","\n","    x, y = np.meshgrid(range(int(-patch_size / 2),int(patch_size / 2)), range(int(-patch_size / 2),int(patch_size / 2)))\n","    x = x.astype(np.float32)\n","    y = y.astype(np.float32)\n","    r = np.sqrt(x*x + y*y) + 0.0001\n","    RMIN = 3  # min allowable ridge spacing\n","    RMAX = 18 # maximum allowable ridge spacing\n","    FLOW = patch_size / RMAX\n","    FHIGH = patch_size / RMIN\n","    dRLow = 1. / (1 + (r / FHIGH) ** 4)\n","    dRHigh = 1. / (1 + (FLOW / r) ** 4)\n","    dBPass = dRLow * dRHigh  # bandpass\n","\n","    dir = np.arctan2(y,x)\n","    dir[dir<0] = dir[dir<0] + math.pi\n","    dir_ind = np.floor(dir/(math.pi/nrof_dirs))\n","    dir_ind = dir_ind.astype(np.int,copy=False)\n","    dir_ind[dir_ind==nrof_dirs] = 0\n","    dir_ind_list = []\n","    for i in range(nrof_dirs):\n","        tmp = np.argwhere(dir_ind == i)\n","        dir_ind_list.append(tmp)\n","    sigma = patch_size/3\n","    weight = np.exp(-(x*x + y*y)/(sigma*sigma))\n","    for i in range(0,blkH):\n","        for j in range(0,blkW):\n","            patch =img[i*block_size:i*block_size+patch_size,j*block_size:j*block_size+patch_size].copy()\n","            local_info[i,j] = local_STFT(patch,weight,dBPass)\n","            local_info[i, j].analysis(r,dir_ind_list)\n","    # get the ridge flow from the local information\n","    dir_map,fre_map = get_ridge_flow_top(local_info)\n","    dir_map = smooth_dir_map(dir_map)\n","\n","    return dir_map, fre_map\n","\n","\n","#thresh=0.5\n","def label2mnt(mnt_s_out, mnt_w_out, mnt_h_out, mnt_o_out, thresh=0.1):\n","    mnt_s_out = np.squeeze(mnt_s_out)\n","    mnt_w_out = np.squeeze(mnt_w_out)\n","    mnt_h_out = np.squeeze(mnt_h_out)\n","    mnt_o_out = np.squeeze(mnt_o_out)\n","    assert len(mnt_s_out.shape)==2 and len(mnt_w_out.shape)==3 and len(mnt_h_out.shape)==3 and len(mnt_o_out.shape)==3\n","\n","    # get cls results\n","    mnt_sparse = sparse.coo_matrix(mnt_s_out>thresh)\n","    mnt_list = np.array(list(zip(mnt_sparse.row, mnt_sparse.col)), dtype=np.int32)\n","    if mnt_list.shape[0] == 0:\n","        return np.zeros((0, 4))\n","\n","    # get regression results\n","    mnt_w_out = np.argmax(mnt_w_out, axis=-1)\n","    mnt_h_out = np.argmax(mnt_h_out, axis=-1)\n","    mnt_o_out = np.argmax(mnt_o_out, axis=-1) # TODO: use ori_highest_peak(np version)\n","\n","    # get final mnt\n","    mnt_final = np.zeros((len(mnt_list), 4))\n","    mnt_final[:, 0] = mnt_sparse.col*8 + mnt_w_out[mnt_list[:,0], mnt_list[:,1]]\n","    mnt_final[:, 1] = mnt_sparse.row*8 + mnt_h_out[mnt_list[:,0], mnt_list[:,1]]\n","    mnt_final[:, 2] = (mnt_o_out[mnt_list[:,0], mnt_list[:,1]]*2-89.)/180*np.pi\n","    mnt_final[mnt_final[:, 2]<0.0, 2] = mnt_final[mnt_final[:, 2]<0.0, 2]+2*np.pi\n","    # New one\n","    mnt_final[:, 2] = (-mnt_final[:, 2]) % (2*np.pi)\n","    mnt_final[:, 3] = mnt_s_out[mnt_list[:,0], mnt_list[:, 1]]\n","\n","    return mnt_final\n","\n","def py_cpu_nms(det, thresh):\n","    if det.shape[0]==0:\n","        return det\n","    dets = det.tolist()\n","    dets.sort(key=lambda x:x[3], reverse=True)\n","    dets = np.array(dets)\n","\n","\n","    box_sz = 25\n","    x1 = np.reshape(dets[:,0],[-1,1]) -box_sz\n","    y1 = np.reshape(dets[:,1],[-1,1]) -box_sz\n","    x2 = np.reshape(dets[:,0],[-1,1]) +box_sz\n","    y2 = np.reshape(dets[:,1],[-1,1]) +box_sz\n","    scores = dets[:, 2]\n","\n","    areas = (x2 - x1 + 1) * (y2 - y1 + 1)\n","    order = scores.argsort()[::-1]\n","\n","    keep = []\n","    while order.size > 0:\n","        i = order[0]\n","        keep.append(i)\n","        xx1 = np.maximum(x1[i], x1[order[1:]])\n","        yy1 = np.maximum(y1[i], y1[order[1:]])\n","        xx2 = np.minimum(x2[i], x2[order[1:]])\n","        yy2 = np.minimum(y2[i], y2[order[1:]])\n","\n","        w = np.maximum(0.0, xx2 - xx1 + 1)\n","        h = np.maximum(0.0, yy2 - yy1 + 1)\n","        inter = w * h\n","        ovr = inter / (areas[i] + areas[order[1:]] - inter)\n","\n","        inds = np.where(ovr <= thresh)[0]\n","        order = order[inds + 1]\n","\n","    return dets[keep, :]\n","\n","def distance(y_true, y_pred, max_D=16, max_O=np.pi/6):\n","    D = spatial.distance.cdist(y_true[:, :2], y_pred[:, :2], 'euclidean')\n","    O = spatial.distance.cdist(np.reshape(y_true[:, 2], [-1, 1]), np.reshape(y_pred[:, 2], [-1, 1]), angle_delta)\n","    return (D<=max_D)*(O<=max_O)\n","\n","def angle_delta(A, B, max_D=np.pi*2):\n","    delta = np.abs(A - B)\n","    delta = np.minimum(delta, max_D-delta)\n","    return delta \n","def nms(mnt):\n","    if mnt.shape[0]==0:\n","        return mnt\n","    # sort score\n","    mnt_sort = mnt.tolist()\n","    mnt_sort.sort(key=lambda x:x[3], reverse=True)\n","    mnt_sort = np.array(mnt_sort)\n","    # cal distance\n","    inrange = distance(mnt_sort, mnt_sort, max_D=16, max_O=np.pi/6).astype(np.float32)\n","    keep_list = np.ones(mnt_sort.shape[0])\n","    for i in range(mnt_sort.shape[0]):\n","        if keep_list[i] == 0:\n","            continue\n","        keep_list[i+1:] = keep_list[i+1:]*(1-inrange[i, i+1:])\n","    return mnt_sort[keep_list.astype(np.bool), :]\n","\n","def fuse_nms(mnt, mnt_set_2):\n","    if mnt.shape[0]==0:\n","        return mnt\n","    # sort score\n","    all_mnt = np.concatenate((mnt, mnt_set_2))\n","\n","    mnt_sort = all_mnt.tolist()\n","    mnt_sort.sort(key=lambda x:x[3], reverse=True)\n","    mnt_sort = np.array(mnt_sort)\n","    # cal distance\n","    inrange = distance(mnt_sort, mnt_sort, max_D=16, max_O=2*np.pi).astype(np.float32)\n","    keep_list = np.ones(mnt_sort.shape[0])\n","    for i in range(mnt_sort.shape[0]):\n","        if keep_list[i] == 0:\n","            continue\n","        keep_list[i+1:] = keep_list[i+1:]*(1-inrange[i, i+1:])\n","    return mnt_sort[keep_list.astype(np.bool), :]\n","\n","def show_orientation_field(img,dir_map,mask=None,fname=None):\n","    h,w = img.shape[:2]\n","\n","    if mask is None:\n","        mask = np.ones((h,w),dtype=np.uint8)\n","    blkH, blkW = dir_map.shape\n","\n","    blk_size = h/blkH\n","\n","    R = blk_size/2*0.8\n","    fig, ax = plt.subplots(1)\n","    ax.imshow(img, cmap='gray')\n","    for i in range(blkH):\n","        y0 = i*blk_size + blk_size/2\n","        y0 = int(y0)\n","        for j in range(blkW):\n","            x0 = j*blk_size + blk_size/2\n","            x0 = int(x0)\n","            ori = dir_map[i,j]\n","            if mask[y0,x0] == 0:\n","                continue\n","            if ori<-9:\n","                continue\n","            x1 = x0 - R * math.cos(ori)\n","            x2 = x0 + R * math.cos(ori)\n","            y1 = y0 - R * math.sin(ori)\n","            y2 = y0 + R * math.sin(ori)\n","            plt.plot([x1, x2], [y1, y2], 'b-', lw=2)\n","    plt.axis('off')\n","    if fname is not None:\n","        fig.savefig(fname,dpi = 500, bbox_inches='tight', pad_inches = 0)\n","        plt.close()\n","    else:\n","        plt.show(block=True)\n","def fuse_minu_orientation(dir_map, mnt, mode=1,block_size=16):\n","    # mode is the way to fuse output minutiae with orientation\n","    # 1: use orientation; 2: use minutiae; 3: fuse average\n","    blkH, blkW = dir_map.shape\n","    dir_map = dir_map%(2*np.pi)\n","\n","    if mode == 1:\n","        for k in range(mnt.shape[0]):\n","            # Choose nearest orientation\n","            ori_value = dir_map[int(mnt[k, 1]//block_size),int(mnt[k, 0]//block_size)]\n","            if 0 < mnt[k, 2] and mnt[k, 2] <= np.pi/2:\n","                if 0 < ori_value and ori_value <= np.pi / 2:\n","                    mnt[k, 2] = ori_value\n","                if np.pi / 2 < ori_value and ori_value <= np.pi:\n","                    if (ori_value - mnt[k, 2]) < (np.pi - ori_value + mnt[k, 2]):\n","                        mnt[k, 2] = ori_value\n","                    else:\n","                        mnt[k, 2] = ori_value + np.pi\n","                if np.pi < ori_value and ori_value <= 3*np.pi/2:\n","                    mnt[k, 2] = ori_value - np.pi\n","                if 3*np.pi/2 < ori_value and ori_value <= 2 * np.pi:\n","                    if (np.pi*2 - ori_value + mnt[k, 2]) < (ori_value - np.pi - mnt[k, 2]):\n","                        mnt[k, 2] = ori_value\n","                    else:\n","                        mnt[k, 2] = ori_value - np.pi\n","            if np.pi/2 < mnt[k, 2] and mnt[k, 2] <= np.pi:\n","                if 0 < ori_value and ori_value <= np.pi / 2:\n","                    if (mnt[k, 2] - ori_value) < (np.pi - ori_value + mnt[k, 2]):\n","                        mnt[k, 2] = ori_value\n","                    else:\n","                        mnt[k, 2] = ori_value + np.pi\n","                if np.pi / 2 < ori_value and ori_value <= np.pi:\n","                    mnt[k, 2] = ori_value\n","                if np.pi < ori_value and ori_value <= 3*np.pi/2:\n","                    if (ori_value - mnt[k, 2]) < (mnt[k, 2] - ori_value + np.pi):\n","                        mnt[k, 2] = ori_value\n","                    else:\n","                        mnt[k, 2] = ori_value - np.pi\n","                if 3*np.pi/2 < ori_value and ori_value <= 2 * np.pi:\n","                    mnt[k, 2] = ori_value - np.pi\n","            if np.pi < mnt[k, 2] and mnt[k, 2] <= 3*np.pi/2:\n","                if 0 < ori_value and ori_value <= np.pi / 2:\n","                    mnt[k, 2] = ori_value + np.pi\n","                if np.pi / 2 < ori_value and ori_value <= np.pi:\n","                    if (mnt[k, 2] - ori_value) < (ori_value + np.pi - mnt[k, 2]):\n","                        mnt[k, 2] = ori_value\n","                    else:\n","                        mnt[k, 2] = ori_value + np.pi\n","                if np.pi < ori_value and ori_value <= 3*np.pi/2:\n","                    mnt[k, 2] = ori_value\n","                if 3*np.pi/2 < ori_value and ori_value <= 2 * np.pi:\n","                    if (ori_value - mnt[k, 2]) < (mnt[k, 2] - ori_value + np.pi):\n","                        mnt[k, 2] = ori_value\n","                    else:\n","                        mnt[k, 2] = ori_value - np.pi\n","            if 3*np.pi/2 < mnt[k, 2] and mnt[k, 2] <= 2*np.pi:\n","                if 0 < ori_value and ori_value <= np.pi / 2:\n","                    if (np.pi - mnt[k, 2] + ori_value) < (mnt[k, 2] - np.pi - ori_value):\n","                        mnt[k, 2] = ori_value\n","                    else:\n","                        mnt[k, 2] = ori_value + np.pi\n","                if np.pi / 2 < ori_value and ori_value <= np.pi:\n","                    mnt[k, 2] = ori_value + np.pi\n","                if np.pi < ori_value and ori_value <= 3*np.pi/2:\n","                    if (mnt[k, 2] - ori_value) < (np.pi*2 - mnt[k, 2] + ori_value - np.pi):\n","                        mnt[k, 2] = ori_value\n","                    else:\n","                        mnt[k, 2] = ori_value - np.pi\n","                if 3*np.pi/2 < ori_value and ori_value <= 2 * np.pi:\n","                    mnt[k, 2] = ori_value\n","\n","\n","    elif mode == 2:\n","        return\n","    elif mode ==3:\n","        for k in range(mnt.shape[0]):\n","            # Choose nearest orientation\n","\n","            ori_value = dir_map[int(mnt[k, 1] // block_size), int(mnt[k, 0] // block_size)]\n","            if 0 < mnt[k, 2] and mnt[k, 2] <= np.pi / 2:\n","                if 0 < ori_value and ori_value <= np.pi / 2:\n","                    fixed_ori = ori_value\n","                if np.pi / 2 < ori_value and ori_value <= np.pi:\n","                    if (ori_value - mnt[k, 2]) < (np.pi - ori_value + mnt[k, 2]):\n","                        fixed_ori = ori_value\n","                    else:\n","                        fixed_ori = ori_value + np.pi\n","                if np.pi < ori_value and ori_value <= 3 * np.pi / 2:\n","                    fixed_ori = ori_value - np.pi\n","                if 3 * np.pi / 2 < ori_value and ori_value <= 2 * np.pi:\n","                    if (np.pi * 2 - ori_value + mnt[k, 2]) < (ori_value - np.pi - mnt[k, 2]):\n","                        fixed_ori = ori_value\n","                    else:\n","                        fixed_ori = ori_value - np.pi\n","            if np.pi / 2 < mnt[k, 2] and mnt[k, 2] <= np.pi:\n","                if 0 < ori_value and ori_value <= np.pi / 2:\n","                    if (mnt[k, 2] - ori_value) < (np.pi - ori_value + mnt[k, 2]):\n","                        fixed_ori = ori_value\n","                    else:\n","                        fixed_ori = ori_value + np.pi\n","                if np.pi / 2 < ori_value and ori_value <= np.pi:\n","                    fixed_ori = ori_value\n","                if np.pi < ori_value and ori_value <= 3 * np.pi / 2:\n","                    if (ori_value - mnt[k, 2]) < (mnt[k, 2] - ori_value + np.pi):\n","                        fixed_ori = ori_value\n","                    else:\n","                        fixed_ori = ori_value - np.pi\n","                if 3 * np.pi / 2 < ori_value and ori_value <= 2 * np.pi:\n","                    fixed_ori = ori_value - np.pi\n","            if np.pi < mnt[k, 2] and mnt[k, 2] <= 3 * np.pi / 2:\n","                if 0 < ori_value and ori_value <= np.pi / 2:\n","                    fixed_ori = ori_value + np.pi\n","                if np.pi / 2 < ori_value and ori_value <= np.pi:\n","                    if (mnt[k, 2] - ori_value) < (ori_value + np.pi - mnt[k, 2]):\n","                        fixed_ori = ori_value\n","                    else:\n","                        fixed_ori = ori_value + np.pi\n","                if np.pi < ori_value and ori_value <= 3 * np.pi / 2:\n","                    fixed_ori = ori_value\n","                if 3 * np.pi / 2 < ori_value and ori_value <= 2 * np.pi:\n","                    if (ori_value - mnt[k, 2]) < (mnt[k, 2] - ori_value + np.pi):\n","                        fixed_ori = ori_value\n","                    else:\n","                        fixed_ori = ori_value - np.pi\n","            if 3 * np.pi / 2 < mnt[k, 2] and mnt[k, 2] <= 2 * np.pi:\n","                if 0 < ori_value and ori_value <= np.pi / 2:\n","                    if (np.pi - mnt[k, 2] + ori_value) < (mnt[k, 2] - np.pi - ori_value):\n","                        fixed_ori = ori_value\n","                    else:\n","                        fixed_ori = ori_value + np.pi\n","                if np.pi / 2 < ori_value and ori_value <= np.pi:\n","                    fixed_ori = ori_value + np.pi\n","                if np.pi < ori_value and ori_value <= 3 * np.pi / 2:\n","                    if (mnt[k, 2] - ori_value) < (np.pi * 2 - mnt[k, 2] + ori_value - np.pi):\n","                        fixed_ori = ori_value\n","                    else:\n","                        fixed_ori = ori_value - np.pi\n","                if 3 * np.pi / 2 < ori_value and ori_value <= 2 * np.pi:\n","                    fixed_ori = ori_value\n","\n","            mnt[k, 2] = (mnt[k, 2] + fixed_ori)/2.0\n","    else:\n","        return\n","\n","\n","def gausslabel(length=180, stride=2):\n","    gaussian_pdf = signal.gaussian(length+1, 3)\n","    label = np.reshape(np.arange(stride/2, length, stride), [1,1,-1,1])\n","    y = np.reshape(np.arange(stride/2, length, stride), [1,1,1,-1])\n","    delta = np.array(np.abs(label - y), dtype=int)\n","    delta = np.minimum(delta, length-delta)+int(length/2)\n","    return gaussian_pdf[delta]\n","def select_max(x):\n","    x = x / (K.max(x, axis=-1, keepdims=True)+K.epsilon())\n","    x = K.tf.where(K.tf.greater(x, 0.999), x, K.tf.zeros_like(x)) # select the biggest one\n","    x = x / (K.sum(x, axis=-1, keepdims=True)+K.epsilon()) # prevent two or more ori is selected\n","    return x\n","def merge_mul(x):\n","    return reduce(lambda x,y:x*y, x)\n","def merge_sum(x):\n","    return reduce(lambda x,y:x+y, x)\n","def reduce_sum(x):\n","    return K.sum(x,axis=-1,keepdims=True)\n","kernal2angle = np.reshape(np.arange(1, 180, 2, dtype=float), [1,1,1,90])/90.*np.pi #2angle = angle*2\n","sin2angle, cos2angle = np.sin(kernal2angle), np.cos(kernal2angle)\n","def ori2angle(ori):\n","    sin2angle_ori = K.sum(ori*sin2angle, -1, keepdims=True)\n","    cos2angle_ori = K.sum(ori*cos2angle, -1, keepdims=True)\n","    modulus_ori = K.sqrt(K.square(sin2angle_ori)+K.square(cos2angle_ori))\n","    return sin2angle_ori, cos2angle_ori, modulus_ori\n","\n","# Group with depth\n","def merge_concat(x):\n","    return K.tf.concat(x,3)\n","\n","def ori_highest_peak(y_pred, length=180):\n","    glabel = gausslabel(length=length,stride=2).astype(np.float32)\n","    y_pred = tf.convert_to_tensor(y_pred, np.float32)\n","    ori_gau = K.conv2d(y_pred,glabel,padding='same')\n","    return ori_gau\n","\n","def img_normalization(img_input, m0=0.0, var0=1.0):\n","    m = K.mean(img_input, axis=[1,2,3], keepdims=True)\n","    var = K.var(img_input, axis=[1,2,3], keepdims=True)\n","    after = K.sqrt(var0*K.tf.square(img_input-m)/var)\n","    image_n = K.tf.where(K.tf.greater(img_input, m), m0+after, m0-after)\n","    return image_n\n","\n","\n","\n","\n","\n","def gabor_bank(stride=2,Lambda=8):\n","\n","    filters_cos = np.ones([25,25,int(180/stride)], dtype=float)\n","    filters_sin = np.ones([25,25,int(180/stride)], dtype=float)\n","\n","    for n, i in enumerate(range(-90,90,stride)):\n","        theta = i*np.pi/180.\n","        kernel_cos, kernel_sin = gabor_fn((24,24),4.5, -theta, Lambda, 0, 0.5)\n","        filters_cos[..., n] = kernel_cos\n","        filters_sin[..., n] = kernel_sin\n","\n","    filters_cos = np.reshape(filters_cos,[25,25,1,-1])\n","    filters_sin = np.reshape(filters_sin,[25,25,1,-1])\n","    return filters_cos, filters_sin\n","\n","def atan2(y_x):\n","    y, x = y_x[0], y_x[1]+K.epsilon()\n","    atan = K.tf.atan(y/x)\n","    angle = K.tf.where(K.tf.greater(x,0.0), atan, K.tf.zeros_like(x))\n","    angle = K.tf.where(K.tf.logical_and(K.tf.less(x,0.0),  K.tf.greater_equal(y,0.0)), atan+np.pi, angle)\n","    angle = K.tf.where(K.tf.logical_and(K.tf.less(x,0.0),  K.tf.less(y,0.0)), atan-np.pi, angle)\n","    return angle\n","def get_document_id(file_name): \n","    return file_name.split('.')[0]\n","#def tocsv(mnt_path=None):\n","#    s=mnt_path+'/csv_results/'\n","#    \n","#    mkdir(s)\n","#    print(os.listdir(mnt_path))\n"," #      if file.endswith('.mnt'):\n","  #          print(file)\n","   #         print(\"found\")\n","    #        data = np.loadtxt(file, skiprows=2)\n","     ####    f_name=s+f_name\n","         #   np.savetxt(f_name, data, delimiter = ',')\n","   # return\n","\n","def work_CNAI(mnt_path=None):\n","    \n","# Start from the first cell. Rows and columns are zero indexed.\n","    row = 0\n","    col = 0\n","# Create a workbook and add a worksheet.\n","    workbook = xlsxwriter.Workbook('MatchResult.xlsx')\n","    worksheet = workbook.add_worksheet()\n","    worksheet.write(0, 0, 'Query FP')\n","    worksheet.write(0, 1, 'Pred FP')\n","    l = CNAI.CNAI()\n","    index_table = dict()\n","    data = dict()\n","    path = mnt_path #+'/csv_results/'\n","    #print(path)\n","    file_list = os.listdir(path)\n","    os.chdir(path)\n","\n","    for file in file_list:\n","        #print(file)\n","        infilename = file\n","        newname = infilename.replace('.mnt', '.json')\n","        print(newname)\n","        doc_id=0\n","        point_id = 0\n","        a=0\n","        b=0\n","        x_try=0\n","\n","        if file.endswith('.mnt'):\n","            doc_id = get_document_id(file)\n","            docid=doc_id\n","            #print(docid)\n","            l.points(file)\n","            point_id = 0\n","            #print(l.pts)\n","            for pt in l.pts:\n","                point_id = point_id + 1\n","                #print(doc_id)\n","                #print(point_id)\n","                #print(pt)\n","                #print(l.pts)\n","                m_neighbours = l.m_neighbours(l.sort_neighbours(pt,l.n_neighbours(pt)))\n","                for m in m_neighbours:\n","                    desc = l.discreet_affine_vector(l.compute_descriptor(l.f_neighbours(m)))\n","                    index = l.compute_index(desc)\n","                    data['doc_id'] = doc_id\n","                    data['point_id'] = point_id\n","                    data['desc'] = desc\n","                    index_table[index] = data\n","                    data = dict()\n","\n","   \n","        root1=os.getcwd() \n","                  \n","        #with open('test.json', 'w') as fp: newname\n","        with open(newname, 'w') as fp: \n","            json.dump(index_table, fp, sort_keys=True, indent=4)\n","        os.chdir(root)\n","        model_path=cnai_model_dir\n","        num_classes=258 # For NIST\n","        #num_classes=10 # For FVC\n","        model = Sequential()\n","        model.add(Conv1D(64, kernel_size=3, activation='relu', input_shape=(15,1)))\n","        model.add(Conv1D(64, kernel_size=3, activation='relu'))\n","        model.add(Conv1D(128, kernel_size=3, activation='relu'))\n","        model.add(Conv1D(128, kernel_size=3, activation='relu'))\n","        model.add(Conv1D(256, kernel_size=3, activation='relu'))\n","        model.add(Conv1D(256, kernel_size=3, activation='relu'))\n","        model.add(MaxPooling1D(pool_size=2))\n","        model.add(Flatten())\n","        model.add(Dense(100, activation='relu'))\n","        model.add(Dense(num_classes, activation='softmax'))\n","        model.summary()\n","        model.load_weights(model_path)\n","        os.chdir(root1)\n","        #with open('test.json', 'r') as test1:\n","        with open(newname, 'r') as test1:\n","            test_data = json.load(test1)\n","\n","        x_try = []\n","        for i in test_data.keys():\n","            x_try.append(test_data[i]['desc'])\n","            \n","        num_test_samples = len(x_try)\n","        time_len=15\n","        num_channels=1\n","        x_try = np.array(x_try)\n","        #print(\"input shape\")\n","        print(x_try.shape)\n","\n","        x_try = x_try.reshape(num_test_samples,time_len, num_channels)\n","        p = model.predict(x_try) \n","        p = np.argmax(p, axis = 1) \n","\n","        p=p+1  # For NIST only\n","        #print(p)  ##\n","\n","        a=collections.Counter(p)\n","        print(a)\n","        #print(Counter(p).items())\n","        print(\"\\nQuery Fingerprint: \")  ##\n","        print(docid)\n","        print(\"\\nPredicted Fingerprint belong to: \")  ##\n","        akey=Counter(a)\n","        ak=akey.most_common(1)\n","        #print(ak)  ##\n","        FP=ak[0][0]\n","        print(FP)\n","        a=docid\n","        b=FP\n","    # Iterate over the data and write it out row by row.\n","        row += 1\n","    #print(row)\n","        worksheet.write(row, col,     a)\n","        worksheet.write(row, col + 1, b)\n","        if a != b: \n","        #print(\"\\nMatch: \")\n","            worksheet.write(row, col + 2, 1)\n","        else:\n","        #print(\"\\nNo Match: \")\n","            worksheet.write(row, col + 2, 0)\n","        a=0\n","        b=0\n","        akey=0\n","        ak=0\n","        FP=0\n","        x_try=0\n","        fp.close()\n","        test1.close()\n","    workbook.close()\n","    return\n","\n","def gabor_fn(ksize, sigma, theta, Lambda, psi, gamma):\n","    sigma_x = sigma\n","    sigma_y = float(sigma) / gamma\n","    # Bounding box\n","    nstds = 3\n","    xmax = ksize[0]/2\n","    ymax = ksize[1]/2\n","    xmin = -xmax\n","    ymin = -ymax\n","    (y, x) = np.meshgrid(np.arange(ymin, ymax + 1), np.arange(xmin, xmax + 1))\n","    # Rotation\n","    x_theta = x * np.cos(theta) + y * np.sin(theta)\n","    y_theta = -x * np.sin(theta) + y * np.cos(theta)\n","    gb_cos = np.exp(-.5 * (x_theta ** 2 / sigma_x ** 2 + y_theta ** 2 / sigma_y ** 2)) * np.cos(2 * np.pi / Lambda * x_theta + psi)\n","    gb_sin = np.exp(-.5 * (x_theta ** 2 / sigma_x ** 2 + y_theta ** 2 / sigma_y ** 2)) * np.sin(2 * np.pi / Lambda * x_theta + psi)\n","    return gb_cos, gb_sin\n","\n","class local_STFT:\n","    def __init__(self,patch,weight = None, dBPass = None):\n","\n","\n","        if weight is not None:\n","            patch = patch * weight\n","        patch = patch - np.mean(patch)\n","        norm = np.linalg.norm(patch)\n","        patch = patch / (norm+0.000001)\n","\n","        f = np.fft.fft2(patch)\n","        fshift = np.fft.fftshift(f)\n","        if dBPass is not None:\n","            fshift = dBPass * fshift\n","\n","        self.patch_FFT = fshift\n","        self.patch = patch\n","        self.ori = None\n","        self.fre = None\n","        self.confidence = None\n","        self.patch_size = patch.shape[0]\n","\n","    def analysis(self,r,dir_ind_list=None,N=2):\n","\n","        assert(dir_ind_list is not None)\n","        energy = np.abs(self.patch_FFT)\n","        energy = energy / (np.sum(energy)+0.00001)\n","        nrof_dirs = len(dir_ind_list)\n","\n","        ori_interval = math.pi/nrof_dirs\n","        ori_interval2 = ori_interval/2\n","\n","\n","        pad_size = 1\n","        dir_norm = np.zeros((nrof_dirs + 2,))\n","        for i in range(nrof_dirs):\n","            tmp = energy[dir_ind_list[i][:, 0], dir_ind_list[i][:, 1]]\n","            dir_norm[i + 1] = np.sum(tmp)\n","\n","        dir_norm[0] = dir_norm[nrof_dirs]\n","        dir_norm[nrof_dirs + 1] = dir_norm[1]\n","\n","        # smooth dir_norm\n","        smoothed_dir_norm = dir_norm\n","        for i in range(1, nrof_dirs + 1):\n","            smoothed_dir_norm[i] = (dir_norm[i - 1] + dir_norm[i] * 4 + dir_norm[i + 1]) / 6\n","\n","        smoothed_dir_norm[0] = smoothed_dir_norm[nrof_dirs]\n","        smoothed_dir_norm[nrof_dirs + 1] = smoothed_dir_norm[1]\n","\n","        den = np.sum(smoothed_dir_norm[1:nrof_dirs + 1]) + 0.00001  # verify if den == 1\n","        smoothed_dir_norm = smoothed_dir_norm/den  # normalization if den == 1, this line can be removed\n","\n","        ori = []\n","        fre = []\n","        confidence = []\n","\n","        wenergy = energy*r\n","        for i in range(1, nrof_dirs+1):\n","            if smoothed_dir_norm[i] > smoothed_dir_norm[i-1] and smoothed_dir_norm[i] > smoothed_dir_norm[i+1]:\n","                tmp_ori = (i-pad_size)*ori_interval + ori_interval2 + math.pi/2\n","                ori.append(tmp_ori)\n","                confidence.append(smoothed_dir_norm[i])\n","                tmp_fre = np.sum(wenergy[dir_ind_list[i-pad_size][:, 0], dir_ind_list[i-pad_size][:, 1]])/dir_norm[i]\n","                tmp_fre = 1/(tmp_fre+0.00001)\n","                fre.append(tmp_fre)\n","\n","\n","        if len(confidence)>0:\n","            confidence = np.asarray(confidence)\n","            fre = np.asarray(fre)\n","            ori = np.asarray(ori)\n","            ind = confidence.argsort()[::-1]\n","            confidence = confidence[ind]\n","            fre = fre[ind]\n","            ori = ori[ind]\n","            if len(confidence) >= 2 and confidence[0]/confidence[1]>2.0:\n","\n","                self.ori = [ori[0]]\n","                self.fre = [fre[0]]\n","                self.confidence = [confidence[0]]\n","            elif len(confidence)>N:\n","                fre = fre[:N]\n","                ori = ori[:N]\n","                confidence = confidence[:N]\n","                self.ori = ori\n","                self.fre = fre\n","                self.confidence = confidence\n","            else:\n","                self.ori = ori\n","                self.fre = fre\n","                self.confidence = confidence\n","\n","    def get_features_of_topN(self,N=2):\n","        if self.confidence is None:\n","            self.border_wave = None\n","            return\n","        candi_num = len(self.ori)\n","        candi_num = np.min([candi_num,N])\n","        patch_size = self.patch_FFT.shape\n","        for i in range(candi_num):\n","\n","            kernel = gabor_kernel(self.fre[i], theta=self.ori[i], sigma_x=10, sigma_y=10)\n","\n","            kernel_f = np.fft.fft2(kernel.real, patch_size)\n","            kernel_f = np.fft.fftshift(kernel_f)\n","            patch_f = self.patch_FFT * kernel_f\n","\n","            patch_f = np.fft.ifftshift(patch_f)  # *np.sqrt(np.abs(fshift)))\n","            rec_patch = np.real(np.fft.ifft2(patch_f))\n","\n","\n","            plt.subplot(121), plt.imshow(self.patch, cmap='gray')\n","            plt.title('Input patch'), plt.xticks([]), plt.yticks([])\n","            plt.subplot(122), plt.imshow(rec_patch, cmap='gray')\n","            plt.title('filtered patch'), plt.xticks([]), plt.yticks([])\n","            plt.show()\n","\n","    def reconstruction(self,weight=None):\n","        f_ifft = np.fft.ifftshift(self.patch_FFT)  # *np.sqrt(np.abs(fshift)))\n","        rec_patch = np.real(np.fft.ifft2(f_ifft))\n","        if weight is not None:\n","            rec_patch = rec_patch * weight\n","        return rec_patch\n","\n","    def gabor_filtering(self,theta,fre,weight=None):\n","\n","        patch_size = self.patch_FFT.shape\n","        kernel = gabor_kernel(fre, theta=theta,sigma_x=4,sigma_y=4)\n","\n","        f = kernel.real\n","        f = f - np.mean(f)\n","        f = f / (np.linalg.norm(f)+0.0001)\n","\n","\n","        kernel_f = np.fft.fft2(f,patch_size)\n","        kernel_f = np.fft.fftshift(kernel_f)\n","        patch_f = self.patch_FFT*kernel_f\n","\n","        patch_f = np.fft.ifftshift(patch_f)  # *np.sqrt(np.abs(fshift)))\n","        rec_patch = np.real(np.fft.ifft2(patch_f))\n","        if weight is not None:\n","            rec_patch = rec_patch * weight\n","        return rec_patch\n","\n","if __name__ =='__main__':\n","    main()\n","    #a=output_dir+\"/CoarseNet_test/mnt_results\"\n","    #work_CNAI(mnt_path=a)\n","\n"],"execution_count":14,"outputs":[{"output_type":"stream","text":["WARNING: Logging before flag parsing goes to stderr.\n","W0401 20:03:51.619139 139724744660864 module_wrapper.py:139] From /usr/local/lib/python2.7/dist-packages/keras/backend/tensorflow_backend.py:74: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n","\n","From /usr/local/lib/python2.7/dist-packages/keras/backend/tensorflow_backend.py:74: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n","\n","W0401 20:03:51.620989 139724744660864 module_wrapper.py:139] From /usr/local/lib/python2.7/dist-packages/keras/backend/tensorflow_backend.py:517: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n","\n","From /usr/local/lib/python2.7/dist-packages/keras/backend/tensorflow_backend.py:517: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n","\n","W0401 20:03:51.640496 139724744660864 deprecation.py:323] From <ipython-input-14-6ff8781c5121>:932: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Use tf.where in 2.0, which has the same broadcast rule as np.where\n","From <ipython-input-14-6ff8781c5121>:932: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Use tf.where in 2.0, which has the same broadcast rule as np.where\n","W0401 20:03:51.653182 139724744660864 module_wrapper.py:139] From /usr/local/lib/python2.7/dist-packages/keras/backend/tensorflow_backend.py:4138: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n","\n","From /usr/local/lib/python2.7/dist-packages/keras/backend/tensorflow_backend.py:4138: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n","\n","W0401 20:03:51.675954 139724744660864 module_wrapper.py:139] From /usr/local/lib/python2.7/dist-packages/keras/backend/tensorflow_backend.py:174: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n","\n","From /usr/local/lib/python2.7/dist-packages/keras/backend/tensorflow_backend.py:174: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n","\n","W0401 20:03:51.677335 139724744660864 module_wrapper.py:139] From /usr/local/lib/python2.7/dist-packages/keras/backend/tensorflow_backend.py:190: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n","\n","From /usr/local/lib/python2.7/dist-packages/keras/backend/tensorflow_backend.py:190: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n","\n","W0401 20:03:51.679758 139724744660864 module_wrapper.py:139] From /usr/local/lib/python2.7/dist-packages/keras/backend/tensorflow_backend.py:199: The name tf.is_variable_initialized is deprecated. Please use tf.compat.v1.is_variable_initialized instead.\n","\n","From /usr/local/lib/python2.7/dist-packages/keras/backend/tensorflow_backend.py:199: The name tf.is_variable_initialized is deprecated. Please use tf.compat.v1.is_variable_initialized instead.\n","\n","W0401 20:03:51.698263 139724744660864 module_wrapper.py:139] From /usr/local/lib/python2.7/dist-packages/keras/backend/tensorflow_backend.py:206: The name tf.variables_initializer is deprecated. Please use tf.compat.v1.variables_initializer instead.\n","\n","From /usr/local/lib/python2.7/dist-packages/keras/backend/tensorflow_backend.py:206: The name tf.variables_initializer is deprecated. Please use tf.compat.v1.variables_initializer instead.\n","\n"],"name":"stderr"},{"output_type":"stream","text":["../Dataset/CoarseNet_test/\n"],"name":"stdout"},{"output_type":"stream","text":["W0401 20:03:52.329585 139724744660864 module_wrapper.py:139] From /usr/local/lib/python2.7/dist-packages/keras/backend/tensorflow_backend.py:1834: The name tf.nn.fused_batch_norm is deprecated. Please use tf.compat.v1.nn.fused_batch_norm instead.\n","\n","From /usr/local/lib/python2.7/dist-packages/keras/backend/tensorflow_backend.py:1834: The name tf.nn.fused_batch_norm is deprecated. Please use tf.compat.v1.nn.fused_batch_norm instead.\n","\n","W0401 20:03:52.367516 139724744660864 module_wrapper.py:139] From /usr/local/lib/python2.7/dist-packages/keras/backend/tensorflow_backend.py:133: The name tf.placeholder_with_default is deprecated. Please use tf.compat.v1.placeholder_with_default instead.\n","\n","From /usr/local/lib/python2.7/dist-packages/keras/backend/tensorflow_backend.py:133: The name tf.placeholder_with_default is deprecated. Please use tf.compat.v1.placeholder_with_default instead.\n","\n","W0401 20:03:52.625706 139724744660864 module_wrapper.py:139] From /usr/local/lib/python2.7/dist-packages/keras/backend/tensorflow_backend.py:3976: The name tf.nn.max_pool is deprecated. Please use tf.nn.max_pool2d instead.\n","\n","From /usr/local/lib/python2.7/dist-packages/keras/backend/tensorflow_backend.py:3976: The name tf.nn.max_pool is deprecated. Please use tf.nn.max_pool2d instead.\n","\n","W0401 20:03:56.141948 139724744660864 module_wrapper.py:139] From /usr/local/lib/python2.7/dist-packages/keras/backend/tensorflow_backend.py:2018: The name tf.image.resize_nearest_neighbor is deprecated. Please use tf.compat.v1.image.resize_nearest_neighbor instead.\n","\n","From /usr/local/lib/python2.7/dist-packages/keras/backend/tensorflow_backend.py:2018: The name tf.image.resize_nearest_neighbor is deprecated. Please use tf.compat.v1.image.resize_nearest_neighbor instead.\n","\n"],"name":"stderr"},{"output_type":"stream","text":["0\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python2.7/dist-packages/ipykernel_launcher.py:166: DeprecationWarning: `imread` is deprecated!\n","`imread` is deprecated in SciPy 1.0.0, and will be removed in 1.2.0.\n","Use ``imageio.imread`` instead.\n","/usr/local/lib/python2.7/dist-packages/ipykernel_launcher.py:173: DeprecationWarning: `imread` is deprecated!\n","`imread` is deprecated in SciPy 1.0.0, and will be removed in 1.2.0.\n","Use ``imageio.imread`` instead.\n"],"name":"stderr"},{"output_type":"stream","text":["(0.1, 0.05)\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python2.7/dist-packages/ipykernel_launcher.py:258: DeprecationWarning: `imsave` is deprecated!\n","`imsave` is deprecated in SciPy 1.0.0, and will be removed in 1.2.0.\n","Use ``imageio.imwrite`` instead.\n"],"name":"stderr"},{"output_type":"stream","text":["1\n","(0.0, -0.05)\n","2\n","(0.1, 0.05)\n","3\n","(0.1, 0.05)\n","4\n","(0.0, -0.05)\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"uvdSCkvfQASA","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":1000},"outputId":"0b6b3965-c12b-4339-b9f7-e3a400f9c4a0","executionInfo":{"status":"ok","timestamp":1585771530607,"user_tz":-330,"elapsed":119440,"user":{"displayName":"Uttam Deshpande","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gi0ntRxzuR6q-uoz9PehErcklTqufxtYeGtc1T69w=s64","userId":"14680885720442117314"}}},"source":["    a=output_dir+\"/CoarseNet_test/mnt_results\"\n","    work_CNAI(mnt_path=a)"],"execution_count":15,"outputs":[{"output_type":"stream","text":["1.json\n","_________________________________________________________________\n","Layer (type)                 Output Shape              Param #   \n","=================================================================\n","conv1d_1 (Conv1D)            (None, 13, 64)            256       \n","_________________________________________________________________\n","conv1d_2 (Conv1D)            (None, 11, 64)            12352     \n","_________________________________________________________________\n","conv1d_3 (Conv1D)            (None, 9, 128)            24704     \n","_________________________________________________________________\n","conv1d_4 (Conv1D)            (None, 7, 128)            49280     \n","_________________________________________________________________\n","conv1d_5 (Conv1D)            (None, 5, 256)            98560     \n","_________________________________________________________________\n","conv1d_6 (Conv1D)            (None, 3, 256)            196864    \n","_________________________________________________________________\n","max_pooling1d_1 (MaxPooling1 (None, 1, 256)            0         \n","_________________________________________________________________\n","flatten_1 (Flatten)          (None, 256)               0         \n","_________________________________________________________________\n","dense_1 (Dense)              (None, 100)               25700     \n","_________________________________________________________________\n","dense_2 (Dense)              (None, 258)               26058     \n","=================================================================\n","Total params: 433,774\n","Trainable params: 433,774\n","Non-trainable params: 0\n","_________________________________________________________________\n","(139, 15)\n","Counter({1: 60, 63: 3, 3: 2, 32: 2, 36: 2, 186: 2, 64: 2, 67: 2, 197: 2, 79: 2, 184: 2, 92: 2, 58: 2, 222: 2, 98: 2, 2: 1, 4: 1, 133: 1, 134: 1, 129: 1, 11: 1, 13: 1, 130: 1, 145: 1, 148: 1, 174: 1, 22: 1, 28: 1, 91: 1, 165: 1, 168: 1, 45: 1, 46: 1, 245: 1, 243: 1, 56: 1, 57: 1, 189: 1, 215: 1, 258: 1, 194: 1, 196: 1, 204: 1, 78: 1, 207: 1, 208: 1, 214: 1, 87: 1, 219: 1, 95: 1, 97: 1, 99: 1, 230: 1, 104: 1, 105: 1, 106: 1, 93: 1, 111: 1, 241: 1, 115: 1, 117: 1, 246: 1, 123: 1, 125: 1, 21: 1})\n","\n","Query Fingerprint: \n","1\n","\n","Predicted Fingerprint belong to: \n","1\n","2.json\n","_________________________________________________________________\n","Layer (type)                 Output Shape              Param #   \n","=================================================================\n","conv1d_7 (Conv1D)            (None, 13, 64)            256       \n","_________________________________________________________________\n","conv1d_8 (Conv1D)            (None, 11, 64)            12352     \n","_________________________________________________________________\n","conv1d_9 (Conv1D)            (None, 9, 128)            24704     \n","_________________________________________________________________\n","conv1d_10 (Conv1D)           (None, 7, 128)            49280     \n","_________________________________________________________________\n","conv1d_11 (Conv1D)           (None, 5, 256)            98560     \n","_________________________________________________________________\n","conv1d_12 (Conv1D)           (None, 3, 256)            196864    \n","_________________________________________________________________\n","max_pooling1d_2 (MaxPooling1 (None, 1, 256)            0         \n","_________________________________________________________________\n","flatten_2 (Flatten)          (None, 256)               0         \n","_________________________________________________________________\n","dense_3 (Dense)              (None, 100)               25700     \n","_________________________________________________________________\n","dense_4 (Dense)              (None, 258)               26058     \n","=================================================================\n","Total params: 433,774\n","Trainable params: 433,774\n","Non-trainable params: 0\n","_________________________________________________________________\n","(250, 15)\n","Counter({1: 61, 2: 5, 111: 5, 3: 4, 63: 4, 36: 3, 58: 3, 64: 3, 92: 3, 98: 3, 99: 3, 148: 3, 197: 3, 4: 2, 11: 2, 21: 2, 26: 2, 27: 2, 32: 2, 46: 2, 51: 2, 65: 2, 67: 2, 71: 2, 74: 2, 77: 2, 78: 2, 79: 2, 81: 2, 93: 2, 106: 2, 120: 2, 123: 2, 129: 2, 134: 2, 165: 2, 167: 2, 184: 2, 186: 2, 194: 2, 216: 2, 218: 2, 221: 2, 222: 2, 249: 2, 251: 2, 5: 1, 6: 1, 7: 1, 8: 1, 10: 1, 13: 1, 14: 1, 17: 1, 20: 1, 22: 1, 28: 1, 33: 1, 40: 1, 44: 1, 45: 1, 56: 1, 57: 1, 68: 1, 72: 1, 75: 1, 80: 1, 83: 1, 87: 1, 90: 1, 91: 1, 95: 1, 97: 1, 104: 1, 105: 1, 113: 1, 115: 1, 117: 1, 119: 1, 122: 1, 125: 1, 128: 1, 130: 1, 133: 1, 141: 1, 145: 1, 149: 1, 154: 1, 156: 1, 168: 1, 169: 1, 171: 1, 173: 1, 174: 1, 175: 1, 176: 1, 177: 1, 189: 1, 193: 1, 195: 1, 196: 1, 198: 1, 201: 1, 203: 1, 204: 1, 207: 1, 208: 1, 211: 1, 214: 1, 215: 1, 219: 1, 225: 1, 229: 1, 230: 1, 232: 1, 233: 1, 234: 1, 236: 1, 241: 1, 243: 1, 245: 1, 246: 1, 253: 1, 255: 1, 256: 1, 257: 1, 258: 1})\n","\n","Query Fingerprint: \n","2\n","\n","Predicted Fingerprint belong to: \n","1\n","3.json\n","_________________________________________________________________\n","Layer (type)                 Output Shape              Param #   \n","=================================================================\n","conv1d_13 (Conv1D)           (None, 13, 64)            256       \n","_________________________________________________________________\n","conv1d_14 (Conv1D)           (None, 11, 64)            12352     \n","_________________________________________________________________\n","conv1d_15 (Conv1D)           (None, 9, 128)            24704     \n","_________________________________________________________________\n","conv1d_16 (Conv1D)           (None, 7, 128)            49280     \n","_________________________________________________________________\n","conv1d_17 (Conv1D)           (None, 5, 256)            98560     \n","_________________________________________________________________\n","conv1d_18 (Conv1D)           (None, 3, 256)            196864    \n","_________________________________________________________________\n","max_pooling1d_3 (MaxPooling1 (None, 1, 256)            0         \n","_________________________________________________________________\n","flatten_3 (Flatten)          (None, 256)               0         \n","_________________________________________________________________\n","dense_5 (Dense)              (None, 100)               25700     \n","_________________________________________________________________\n","dense_6 (Dense)              (None, 258)               26058     \n","=================================================================\n","Total params: 433,774\n","Trainable params: 433,774\n","Non-trainable params: 0\n","_________________________________________________________________\n","(409, 15)\n","Counter({1: 62, 17: 6, 2: 5, 3: 5, 4: 5, 51: 5, 63: 5, 65: 5, 106: 5, 111: 5, 128: 5, 197: 5, 36: 4, 56: 4, 64: 4, 92: 4, 98: 4, 123: 4, 148: 4, 172: 4, 186: 4, 198: 4, 251: 4, 11: 3, 16: 3, 27: 3, 28: 3, 40: 3, 58: 3, 67: 3, 72: 3, 73: 3, 79: 3, 80: 3, 84: 3, 99: 3, 104: 3, 105: 3, 119: 3, 125: 3, 129: 3, 134: 3, 139: 3, 165: 3, 177: 3, 184: 3, 199: 3, 218: 3, 221: 3, 258: 3, 7: 2, 8: 2, 10: 2, 21: 2, 26: 2, 32: 2, 37: 2, 38: 2, 46: 2, 68: 2, 71: 2, 74: 2, 77: 2, 78: 2, 81: 2, 83: 2, 93: 2, 95: 2, 115: 2, 120: 2, 122: 2, 130: 2, 137: 2, 141: 2, 143: 2, 145: 2, 154: 2, 167: 2, 168: 2, 170: 2, 194: 2, 201: 2, 203: 2, 208: 2, 216: 2, 219: 2, 222: 2, 233: 2, 246: 2, 249: 2, 5: 1, 6: 1, 9: 1, 13: 1, 14: 1, 15: 1, 20: 1, 22: 1, 24: 1, 25: 1, 33: 1, 42: 1, 44: 1, 45: 1, 47: 1, 52: 1, 57: 1, 59: 1, 61: 1, 69: 1, 75: 1, 86: 1, 87: 1, 90: 1, 91: 1, 96: 1, 97: 1, 102: 1, 107: 1, 108: 1, 113: 1, 117: 1, 118: 1, 127: 1, 131: 1, 133: 1, 142: 1, 146: 1, 147: 1, 149: 1, 155: 1, 156: 1, 157: 1, 159: 1, 164: 1, 169: 1, 171: 1, 173: 1, 174: 1, 175: 1, 176: 1, 180: 1, 183: 1, 189: 1, 190: 1, 191: 1, 193: 1, 195: 1, 196: 1, 200: 1, 204: 1, 207: 1, 210: 1, 211: 1, 214: 1, 215: 1, 225: 1, 227: 1, 228: 1, 229: 1, 230: 1, 232: 1, 234: 1, 236: 1, 239: 1, 241: 1, 242: 1, 243: 1, 244: 1, 245: 1, 250: 1, 252: 1, 253: 1, 255: 1, 256: 1, 257: 1})\n","\n","Query Fingerprint: \n","3\n","\n","Predicted Fingerprint belong to: \n","1\n","4.json\n","_________________________________________________________________\n","Layer (type)                 Output Shape              Param #   \n","=================================================================\n","conv1d_19 (Conv1D)           (None, 13, 64)            256       \n","_________________________________________________________________\n","conv1d_20 (Conv1D)           (None, 11, 64)            12352     \n","_________________________________________________________________\n","conv1d_21 (Conv1D)           (None, 9, 128)            24704     \n","_________________________________________________________________\n","conv1d_22 (Conv1D)           (None, 7, 128)            49280     \n","_________________________________________________________________\n","conv1d_23 (Conv1D)           (None, 5, 256)            98560     \n","_________________________________________________________________\n","conv1d_24 (Conv1D)           (None, 3, 256)            196864    \n","_________________________________________________________________\n","max_pooling1d_4 (MaxPooling1 (None, 1, 256)            0         \n","_________________________________________________________________\n","flatten_4 (Flatten)          (None, 256)               0         \n","_________________________________________________________________\n","dense_7 (Dense)              (None, 100)               25700     \n","_________________________________________________________________\n","dense_8 (Dense)              (None, 258)               26058     \n","=================================================================\n","Total params: 433,774\n","Trainable params: 433,774\n","Non-trainable params: 0\n","_________________________________________________________________\n","(628, 15)\n","Counter({1: 64, 123: 11, 17: 9, 4: 8, 65: 7, 119: 7, 148: 7, 186: 7, 2: 6, 28: 6, 51: 6, 63: 6, 98: 6, 106: 6, 111: 6, 172: 6, 197: 6, 3: 5, 10: 5, 31: 5, 36: 5, 56: 5, 57: 5, 58: 5, 90: 5, 92: 5, 128: 5, 129: 5, 173: 5, 198: 5, 251: 5, 258: 5, 11: 4, 26: 4, 37: 4, 38: 4, 40: 4, 64: 4, 67: 4, 71: 4, 74: 4, 79: 4, 80: 4, 84: 4, 96: 4, 99: 4, 104: 4, 115: 4, 130: 4, 134: 4, 139: 4, 156: 4, 165: 4, 184: 4, 193: 4, 199: 4, 233: 4, 246: 4, 7: 3, 16: 3, 20: 3, 27: 3, 41: 3, 42: 3, 45: 3, 46: 3, 60: 3, 61: 3, 68: 3, 72: 3, 73: 3, 77: 3, 78: 3, 81: 3, 87: 3, 93: 3, 95: 3, 105: 3, 108: 3, 117: 3, 125: 3, 141: 3, 143: 3, 147: 3, 157: 3, 170: 3, 177: 3, 204: 3, 218: 3, 219: 3, 221: 3, 6: 2, 8: 2, 14: 2, 21: 2, 25: 2, 32: 2, 33: 2, 43: 2, 53: 2, 82: 2, 83: 2, 86: 2, 88: 2, 113: 2, 120: 2, 121: 2, 122: 2, 131: 2, 136: 2, 137: 2, 145: 2, 154: 2, 167: 2, 168: 2, 171: 2, 174: 2, 178: 2, 188: 2, 189: 2, 194: 2, 201: 2, 203: 2, 208: 2, 210: 2, 212: 2, 214: 2, 215: 2, 216: 2, 222: 2, 227: 2, 231: 2, 232: 2, 234: 2, 239: 2, 240: 2, 244: 2, 249: 2, 252: 2, 255: 2, 256: 2, 257: 2, 5: 1, 9: 1, 12: 1, 13: 1, 15: 1, 22: 1, 24: 1, 29: 1, 30: 1, 34: 1, 35: 1, 39: 1, 44: 1, 47: 1, 48: 1, 49: 1, 52: 1, 59: 1, 62: 1, 69: 1, 75: 1, 85: 1, 91: 1, 97: 1, 101: 1, 102: 1, 103: 1, 107: 1, 110: 1, 112: 1, 118: 1, 127: 1, 132: 1, 133: 1, 142: 1, 146: 1, 149: 1, 150: 1, 155: 1, 159: 1, 160: 1, 161: 1, 163: 1, 164: 1, 169: 1, 175: 1, 176: 1, 180: 1, 183: 1, 185: 1, 190: 1, 191: 1, 195: 1, 196: 1, 200: 1, 206: 1, 207: 1, 211: 1, 217: 1, 220: 1, 223: 1, 225: 1, 228: 1, 229: 1, 230: 1, 235: 1, 236: 1, 241: 1, 242: 1, 243: 1, 245: 1, 248: 1, 250: 1, 253: 1})\n","\n","Query Fingerprint: \n","4\n","\n","Predicted Fingerprint belong to: \n","1\n","5.json\n","_________________________________________________________________\n","Layer (type)                 Output Shape              Param #   \n","=================================================================\n","conv1d_25 (Conv1D)           (None, 13, 64)            256       \n","_________________________________________________________________\n","conv1d_26 (Conv1D)           (None, 11, 64)            12352     \n","_________________________________________________________________\n","conv1d_27 (Conv1D)           (None, 9, 128)            24704     \n","_________________________________________________________________\n","conv1d_28 (Conv1D)           (None, 7, 128)            49280     \n","_________________________________________________________________\n","conv1d_29 (Conv1D)           (None, 5, 256)            98560     \n","_________________________________________________________________\n","conv1d_30 (Conv1D)           (None, 3, 256)            196864    \n","_________________________________________________________________\n","max_pooling1d_5 (MaxPooling1 (None, 1, 256)            0         \n","_________________________________________________________________\n","flatten_5 (Flatten)          (None, 256)               0         \n","_________________________________________________________________\n","dense_9 (Dense)              (None, 100)               25700     \n","_________________________________________________________________\n","dense_10 (Dense)             (None, 258)               26058     \n","=================================================================\n","Total params: 433,774\n","Trainable params: 433,774\n","Non-trainable params: 0\n","_________________________________________________________________\n","(717, 15)\n","Counter({1: 66, 123: 12, 17: 10, 119: 9, 2: 8, 4: 8, 65: 8, 98: 8, 148: 8, 36: 7, 51: 7, 134: 7, 186: 7, 10: 6, 28: 6, 56: 6, 57: 6, 63: 6, 67: 6, 106: 6, 111: 6, 172: 6, 193: 6, 197: 6, 198: 6, 3: 5, 5: 5, 31: 5, 37: 5, 40: 5, 45: 5, 47: 5, 58: 5, 72: 5, 90: 5, 92: 5, 128: 5, 129: 5, 156: 5, 173: 5, 251: 5, 258: 5, 7: 4, 11: 4, 20: 4, 26: 4, 38: 4, 42: 4, 64: 4, 71: 4, 74: 4, 78: 4, 79: 4, 80: 4, 84: 4, 95: 4, 96: 4, 99: 4, 104: 4, 105: 4, 115: 4, 130: 4, 139: 4, 157: 4, 165: 4, 184: 4, 199: 4, 215: 4, 233: 4, 239: 4, 246: 4, 16: 3, 21: 3, 27: 3, 33: 3, 41: 3, 43: 3, 46: 3, 53: 3, 60: 3, 61: 3, 68: 3, 73: 3, 75: 3, 77: 3, 81: 3, 87: 3, 93: 3, 108: 3, 117: 3, 121: 3, 125: 3, 137: 3, 141: 3, 143: 3, 147: 3, 170: 3, 171: 3, 174: 3, 177: 3, 204: 3, 210: 3, 214: 3, 218: 3, 219: 3, 221: 3, 236: 3, 245: 3, 6: 2, 8: 2, 13: 2, 14: 2, 25: 2, 29: 2, 30: 2, 32: 2, 48: 2, 82: 2, 83: 2, 86: 2, 88: 2, 91: 2, 110: 2, 113: 2, 118: 2, 120: 2, 122: 2, 131: 2, 136: 2, 145: 2, 146: 2, 154: 2, 160: 2, 167: 2, 168: 2, 175: 2, 178: 2, 179: 2, 188: 2, 189: 2, 191: 2, 194: 2, 195: 2, 201: 2, 203: 2, 208: 2, 212: 2, 216: 2, 220: 2, 222: 2, 227: 2, 231: 2, 232: 2, 234: 2, 240: 2, 244: 2, 249: 2, 252: 2, 255: 2, 256: 2, 257: 2, 9: 1, 12: 1, 15: 1, 19: 1, 22: 1, 24: 1, 34: 1, 35: 1, 39: 1, 44: 1, 49: 1, 52: 1, 59: 1, 62: 1, 69: 1, 76: 1, 85: 1, 94: 1, 97: 1, 100: 1, 101: 1, 102: 1, 103: 1, 107: 1, 112: 1, 114: 1, 127: 1, 132: 1, 133: 1, 142: 1, 144: 1, 149: 1, 150: 1, 155: 1, 159: 1, 161: 1, 163: 1, 164: 1, 169: 1, 176: 1, 180: 1, 183: 1, 185: 1, 190: 1, 196: 1, 200: 1, 206: 1, 207: 1, 211: 1, 217: 1, 223: 1, 225: 1, 228: 1, 229: 1, 230: 1, 235: 1, 241: 1, 242: 1, 243: 1, 248: 1, 250: 1, 253: 1})\n","\n","Query Fingerprint: \n","5\n","\n","Predicted Fingerprint belong to: \n","1\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"mX9JywKKJieF","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]}]}